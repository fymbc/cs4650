{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fymbc/cs4650/blob/main/PA2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqxpid8J3_xt"
      },
      "source": [
        "# Part of Speech Tagging with LSTM, Fine-tuned BERT\n",
        "**CS 4650 \"Natural Language Processing\" Project 2**  \n",
        "Georgia Tech, Spring 2025 (Instructor: Weicheng Ma)\n",
        "\n",
        "**To start, first make a copy of this notebook to your local drive, so you can edit it.**\n",
        "\n",
        "If you want GPUs (which will improve training speed), you can always change your instance type to GPU by going to Runtime -> Change runtime type -> Hardware accelerator.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Basic POS Tagger  [15 points]\n",
        "\n",
        "In this assignment, we will train LSTM-based POS-taggers, and evaluate their performance. We will use English text from the Wall Street Journal, marked with POS tags such as `NNP` (proper noun) and `DT` (determiner)."
      ],
      "metadata": {
        "id": "S_DVctvT4zlV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3X367eCR3_x0"
      },
      "source": [
        "### 1.1 Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O train.txt \"https://www.dropbox.com/scl/fi/nqtk53b2ihqzf6hugolms/train.txt?rlkey=y7003b74z7gp06e8qa2gfgd4r&st=37lt5q66&dl=0\""
      ],
      "metadata": {
        "id": "2hVu_ia9Ottg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtnGNDoA3_x3"
      },
      "outputs": [],
      "source": [
        "# ===========================================================================\n",
        "# Run some setup code for this notebook. Don't modify anything in this cell.\n",
        "# ===========================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import random\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "\n",
        "# ===========================================================================\n",
        "# A quick note on CUDA functionality (and `.to(model.device)`):\n",
        "# CUDA is a parallel GPU platform produced by NVIDIA and is used by most GPU\n",
        "# libraries in PyTorch. CUDA organizes GPUs into device IDs (i.e., \"cuda:X\" for GPU #X).\n",
        "# \"device\" will tell PyTorch which GPU (or CPU) to place an object in. Since\n",
        "# collab only uses one GPU, we will use 'cuda' as the device if a GPU is available\n",
        "# and the CPU if not. You will run into problems if your tensors are on different devices.\n",
        "# ===========================================================================\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can check to make sure a GPU is available using the following code block.\n",
        "\n",
        "\n",
        "```py\n",
        "# If the below message is shown, it means you are using a CPU.\n",
        "/bin/bash: nvidia-smi: command not found\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xuEywStkc3M5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "ounnp0ASc58O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwA2y6OR3_yE"
      },
      "source": [
        "### 1.2 Preparing Data\n",
        "\n",
        "`train.txt`: The training data is present in this file. This file contains sequences of words and their respective tags. The data is split into 80% training and 20% development to train the model and tune the hyperparameters, respectively. See `load_tag_data` for details on how to read the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFpH2P1A3_yG"
      },
      "outputs": [],
      "source": [
        "# ===========================================================================\n",
        "# Run some preprocessing code for our dataset. Don't modify anything in this cell.\n",
        "# ===========================================================================\n",
        "\n",
        "def load_tag_data(tag_file):\n",
        "    all_sentences = []\n",
        "    all_tags = []\n",
        "    sent = []\n",
        "    tags = []\n",
        "    with open(tag_file, 'r') as f:\n",
        "        for line in f:\n",
        "            if line.strip() == \"\":\n",
        "                all_sentences.append(sent)\n",
        "                all_tags.append(tags)\n",
        "                sent = []\n",
        "                tags = []\n",
        "            else:\n",
        "                word, tag, _ = line.strip().split()\n",
        "                sent.append(word)\n",
        "                tags.append(tag)\n",
        "    return all_sentences, all_tags\n",
        "\n",
        "train_sentences, train_tags = load_tag_data('train.txt')\n",
        "\n",
        "unique_tags = set([tag for tag_seq in train_tags for tag in tag_seq])\n",
        "\n",
        "# Create train-val split from train data\n",
        "train_val_data = list(zip(train_sentences, train_tags))\n",
        "random.shuffle(train_val_data)\n",
        "split = int(0.8 * len(train_val_data))\n",
        "training_data = train_val_data[:split]\n",
        "val_data = train_val_data[split:]\n",
        "\n",
        "print(\"Train Data: \", len(training_data))\n",
        "print(\"Val Data: \", len(val_data))\n",
        "print(\"Total tags: \", len(unique_tags))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlfliN0J-RzV"
      },
      "source": [
        "### 1.3 Word-to-Index and Tag-to-Index mapping\n",
        "In order to work with text in Tensor format, we need to map each word to an index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uojEDun83_yP"
      },
      "outputs": [],
      "source": [
        "# ===========================================================================\n",
        "# Don't modify anything in this cell.\n",
        "# ===========================================================================\n",
        "\n",
        "word_to_idx = {}\n",
        "for sent in train_sentences:\n",
        "    for word in sent:\n",
        "        if word not in word_to_idx:\n",
        "            word_to_idx[word] = len(word_to_idx)\n",
        "\n",
        "tag_to_idx = {}\n",
        "for tag in unique_tags:\n",
        "    if tag not in tag_to_idx:\n",
        "        tag_to_idx[tag] = len(tag_to_idx)\n",
        "\n",
        "idx_to_tag = {}\n",
        "for tag in tag_to_idx:\n",
        "    idx_to_tag[tag_to_idx[tag]] = tag\n",
        "\n",
        "print(\"Total tags\", len(tag_to_idx))\n",
        "print(\"Vocab size\", len(word_to_idx))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H26dqorp3_yX"
      },
      "outputs": [],
      "source": [
        "def prepare_sequence(sent, idx_mapping):\n",
        "    idxs = [idx_mapping[word] for word in sent]\n",
        "    return torch.tensor(idxs, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRnBTCwD3_yc"
      },
      "source": [
        "### 1.4 Set up model\n",
        "We will build and train a Basic POS Tagger which is an LSTM model to tag the parts of speech in a given sentence. Here we define a few default hyperparameters for your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2P5SHabu3_yf"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_DIM = 4\n",
        "HIDDEN_DIM = 8\n",
        "LEARNING_RATE = 0.1\n",
        "LSTM_LAYERS = 1\n",
        "DROPOUT = 0\n",
        "EPOCHS = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkkS4oEb3_yk"
      },
      "source": [
        "### 1.5 Define Model [5 points]\n",
        "\n",
        "The model takes as input a sentence as a tensor in the index space. This sentence is then converted to embedding space where each word maps to its word embedding. The word embeddings is learned as part of the model training process. These word embeddings act as input to the LSTM which produces a representation for each word. Then the representations of words are passed to a Linear layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCa30HQb3_ym"
      },
      "outputs": [],
      "source": [
        "class BasicPOSTagger(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "        \"\"\"\n",
        "        Define and initialize anything needed for the forward pass.\n",
        "\n",
        "        You are required to create a model with:\n",
        "          an embedding layer: that maps words to the embedding space\n",
        "          an LSTM layer: that takes word embeddings as input and outputs hidden states\n",
        "          a linear layer: maps from hidden state space to tag space\n",
        "        \"\"\"\n",
        "        super(BasicPOSTagger, self).__init__()\n",
        "\n",
        "        ### BEGIN YOUR CODE ###\n",
        "\n",
        "\n",
        "        ### END YOUR CODE ###\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        \"\"\"\n",
        "        Implement the forward pass.\n",
        "\n",
        "        Given a tokenized index-mapped sentence as the argument,\n",
        "        compute the corresponding raw scores for tags (without softmax)\n",
        "\n",
        "        returns:: tag_scores (Tensor)\n",
        "        \"\"\"\n",
        "        tag_scores = None\n",
        "\n",
        "        ### BEGIN YOUR CODE ###\n",
        "\n",
        "\n",
        "        ### END YOUR CODE ###\n",
        "\n",
        "        return tag_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ot9J3MrB3_ys"
      },
      "source": [
        "### 1.6 Training [5 points]\n",
        "\n",
        "We define train and evaluate procedures that allow us to train our model using our created train-val split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWMGxh4Z3_yv"
      },
      "outputs": [],
      "source": [
        "def train(epoch, model, loss_function, optimizer):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_examples = 0\n",
        "    for sentence, tags in training_data:\n",
        "        \"\"\"\n",
        "        Implement the training method\n",
        "\n",
        "        Hint: you can use the prepare_sequence method for creating index mappings\n",
        "        for sentences. Find the gradient with respect to the loss and update the\n",
        "        model parameters using the optimizer.\n",
        "        \"\"\"\n",
        "\n",
        "        ### BEGIN YOUR CODE ###\n",
        "\n",
        "        # Zero out the parameter gradients\n",
        "\n",
        "        # Prepare input data (sentences and gold labels)\n",
        "\n",
        "        # Do forward pass with current batch of input\n",
        "\n",
        "        # Get loss with model predictions and true labels\n",
        "\n",
        "        # Update model parameters\n",
        "\n",
        "        # Increase running total loss and the number of past training samples\n",
        "\n",
        "        ### END YOUR CODE ###\n",
        "\n",
        "    avg_train_loss = train_loss / train_examples\n",
        "    avg_val_loss, val_accuracy = evaluate(model, loss_function)\n",
        "\n",
        "    print(f\"Epoch: {epoch}/{EPOCHS}\\tAvg Train Loss: {avg_train_loss:.4f}\\tAvg Val Loss: {avg_val_loss:.4f}\\t Val Accuracy: {val_accuracy:.0f}\")\n",
        "\n",
        "def evaluate(model, loss_function):\n",
        "    \"\"\"\n",
        "    returns:: avg_val_loss (float)\n",
        "    returns:: val_accuracy (float)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    val_loss = 0\n",
        "    val_examples = 0\n",
        "    with torch.no_grad():\n",
        "        for sentence, tags in val_data:\n",
        "            \"\"\"\n",
        "            Implement the evaluate method\n",
        "\n",
        "            Find the average validation loss along with the validation accuracy.\n",
        "            Hint: To find the accuracy, argmax of tag predictions can be used.s\n",
        "            \"\"\"\n",
        "            ### BEGIN YOUR CODE ###\n",
        "\n",
        "            # Prepare input data (sentences and gold labels)\n",
        "\n",
        "            # Do forward pass with current batch of input\n",
        "\n",
        "            # Get loss with model predictions and true labels\n",
        "\n",
        "            # Get the predicted labels\n",
        "\n",
        "            # Get number of correct prediction\n",
        "\n",
        "            # Increase running total loss and the number of past valid samples\n",
        "\n",
        "            ### END YOUR CODE ###\n",
        "    val_accuracy = 100. * correct / val_examples\n",
        "    avg_val_loss = val_loss / val_examples\n",
        "    return avg_val_loss, val_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsuHjjH1rQeS"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Initialize the model, optimizer and the loss function\n",
        "\"\"\"\n",
        "### BEGIN YOUR CODE ###\n",
        "\n",
        "\n",
        "### END YOUR CODE ###\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train(epoch, model, loss_function, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uK6mT_k8NRvB"
      },
      "source": [
        "*Hint: Under the default hyperparameter setting, after 5 epochs you should be able to get at least `0.75` accuracy on the validation set.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iP64WDReBuDr"
      },
      "source": [
        "### 1.7 Error analysis [5 points]\n",
        "\n",
        "In this step, we will analyze what kind of errors it was making on the validation set.\n",
        "\n",
        "Step 1, write a method to generate predictions from the validation set. For every sentence, get its words, predicted tags (model_tags), and the ground truth tags (gt_tags). To make the next step easier, you may want to concatenate words from all sentences into a very long list, and same for model_tags and gt_tags.\n",
        "\n",
        "\n",
        "Step 2, analyze what kind of errors the model was making. For example, it may frequently label NN as VB. Let's get the top-10 most frequent types of errors, each of their frequency, and some example words. One example is at below. It is interpreted as the model predicts NNP as VBG for 626 times, five random example words are shown.\n",
        "\n",
        "```\n",
        "['VBG', 'NNP', 626, ['Rowe', 'Livermore', 'Parker', 'F-16', 'HEYNOW']]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QgMHr7HCn1x"
      },
      "outputs": [],
      "source": [
        "def generate_predictions(model, val_data):\n",
        "    \"\"\"\n",
        "    Generate predictions for val_data\n",
        "\n",
        "    Create lists of words, tags predicted by the model and ground truth tags.\n",
        "    Hint: It should look very similar to the evaluate function.\n",
        "\n",
        "    returns:: word_list (str list)\n",
        "    returns:: model_tags (str list)\n",
        "    returns:: gt_tags (str list)\n",
        "    \"\"\"\n",
        "    ### BEGIN YOUR CODE ###\n",
        "\n",
        "\n",
        "    ### END YOUR CODE ###\n",
        "\n",
        "    return word_list, model_tags, gt_tags\n",
        "\n",
        "def error_analysis(word_list, model_tags, gt_tags):\n",
        "    \"\"\"\"\n",
        "    Carry out error analysis\n",
        "\n",
        "    From those lists collected from the above method, find the\n",
        "    top-10 tuples of (model_tag, ground_truth_tag, frequency, example words)\n",
        "    sorted by frequency\n",
        "\n",
        "    returns: errors (list of tuples)\n",
        "    \"\"\"\n",
        "    ### BEGIN YOUR CODE ###\n",
        "\n",
        "\n",
        "    ### END YOUR CODE ###\n",
        "\n",
        "    return errors\n",
        "\n",
        "word_list, model_tags, gt_tags = generate_predictions(model, val_data)\n",
        "errors = error_analysis(word_list, model_tags, gt_tags)\n",
        "\n",
        "for i in errors[:10]:\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRNjFRDcD2h7"
      },
      "source": [
        "**Report your findings here.**  \n",
        "What kinds of errors did the model make and why do you think it made them?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLcI9BUZIo04"
      },
      "source": [
        "## 2. Hyper-parameter Tuning [10 points]\n",
        "\n",
        "In order to improve your model performance, try making some modifications on `EMBEDDING_DIM`, `HIDDEN_DIM`, and `LEARNING_RATE`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RekmpLxzIo04"
      },
      "outputs": [],
      "source": [
        "YOUR_EMBEDDING_DIM = None\n",
        "YOUR_HIDDEN_DIM = None\n",
        "YOUR_LEARNING_RATE = None\n",
        "\n",
        "# Set three hyper-parameters. Initialize the model, optimizer and the loss function\n",
        "# Hint, you may want to use reduction='sum' in the CrossEntropyLoss function\n",
        "\n",
        "### BEGIN YOUR CODE ###\n",
        "\n",
        "\n",
        "### END YOUR CODE ###\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train(epoch, model, loss_function, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svXyUssdXZ4r"
      },
      "source": [
        "## 3. Character-level POS Tagger  [15 points]\n",
        "\n",
        "Use the character-level information to augment word embeddings. For example, words that end with -ing or -ly give quite a bit of information about their POS tags. To incorporate this information, run a character-level LSTM on every word to create a character-level representation of the word. Take the last hidden state from the character-level LSTM as the representation and concatenate with the word embedding (as in the `BasicPOSTagger`) to create a new word representation that captures more information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nX4-3AoxSJeY"
      },
      "outputs": [],
      "source": [
        "# Create char to index mapping\n",
        "char_to_idx = {}\n",
        "unique_chars = set()\n",
        "MAX_WORD_LEN = 0\n",
        "\n",
        "for sent in train_sentences:\n",
        "    for word in sent:\n",
        "        for c in word:\n",
        "            unique_chars.add(c)\n",
        "        if len(word) > MAX_WORD_LEN:\n",
        "            MAX_WORD_LEN = len(word)\n",
        "\n",
        "for c in unique_chars:\n",
        "    char_to_idx[c] = len(char_to_idx)\n",
        "char_to_idx[' '] = len(char_to_idx)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### An Aside on Padding\n",
        "\n",
        "#### How to do padding correctly for the characters?\n",
        "\n",
        "\n",
        "Assume we have got a sentence [\"We\", \"love\", \"NLP\"]. You are supposed to first prepend a certain number of blank characters to each of the words in this sentence.\n",
        "\n",
        "How to determine the number of blank characters we need? The calculation of MAX_WORD_LEN is here for help (which we already provide in the starter code). For the given sentence, MAX_WORD_LEN equals 4. Therefore we prepend two blank characters to \"We\", zero blank character to \"love\", and one blank character to \"NLP\". So the resultant padded sentence we get should be [\"  We\", \"love\", \" NLP\"].\n",
        "\n",
        "Then, we feed all characters in [\"  We\", \"love\", \" NLP\"] into a char-embedding layer, and get a tensor of shape (3, 4, char_embedding_dim). To make this tensor's shape proper for the char-level LSTM (nn.LSTM), we need to transpose this tensor, i.e. swap the first and the second dimension. So we get a tensor of shape (4, 3, char_embedding_dim), where 4 corresponds to seq_len and 3 corresponds to batch_size.\n",
        "\n",
        "The last thing you need to do is to obtain the last hidden state from the char-level LSTM, and concatenate it with the word embedding, so that you can get an augmented representation of that word.\n",
        "\n",
        "![padding](https://raw.githubusercontent.com/chaojiang06/chaojiang06.github.io/master/TA/spring2022_CS4650/char_padding.png)\n",
        "  *An illustration for left padding characters*\n",
        "\n",
        "#### Why doing the padding?\n",
        "Someone may ask why we want to do such a kind of padding, instead of directly passing each of the character sequences of each word one by one through an LSTM, to get the last hidden state. The reason is that if you don't do padding, then that means you can only implement this process using \"for loop\". For CharPOSTagger, if you implement it using \"for loop\", the training time would be approximately 150s (GPU) / 250s (CPU) per epoch, while it would be around 30s (GPU) / 150s (CPU) per epoch if you do the padding and feed your data in batches. Therefore, we strongly recommend you learn how to do the padding and transform your data into batches. In fact, those are quite important concepts which you should get yourself familar with, although it might take you some time.\n",
        "\n",
        "#### Why doing *left* padding?\n",
        "Our hypothesis is that the suffixes of English words (e.g., -ly, -ing, etc) are more indicative than prefixes for the part-of-speech (POS). Though LSTM is supposed to be able to handle long sequences, it still lose information along the way and the information closer to the last state (which you use as char-level representations) will be retained better.\n",
        "\n",
        "#### How to understand the dimention change?\n",
        "Assume we have got a sentence with 3 words [\"We\", \"love\", \"NLP\"], and assume the dimension of character embedding is 2, the dimension of word embedding is 4, the dimension of word-level LSTM's hidden layer is 5, the dimension of character-level LSTM's hidden layer is 6.\n",
        "\n",
        "In `BasicPOSTagger`, the dimension change would be:\n",
        "\n",
        "- ------ input ------> $(3\\times 1\\times 4)$\n",
        "- -- word-level LSTM --> $(3\\times 1\\times 5)$\n",
        "- ----- linear layer -----> $(3\\times 1\\times 44)$\n",
        "\n",
        "In `CharPOSTagger`, after padding, character embedding, and swapping, the dimension change would be:\n",
        "\n",
        "- ------ input ------> $(\\text{MAX_WORD_LEN}\\times 3\\times 2)$\n",
        "-  -- character-level LSTM --> $(\\text{MAX_WORD_LEN}\\times 3\\times 6)$\n",
        "- -- Take the last hidden state --> $(3\\times 6)$\n",
        "- -- concatenate with word embedings --> $(3\\times 1\\times 10)$\n",
        "- -- word-level LSTM --> $(3\\times 1\\times 5)$\n",
        "- -- linear layer --> $(3\\times 1\\times 44)$."
      ],
      "metadata": {
        "id": "8xXPsL3nDjAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM = 4\n",
        "HIDDEN_DIM = 8\n",
        "LEARNING_RATE = 0.1\n",
        "LSTM_LAYERS = 1\n",
        "DROPOUT = 0\n",
        "EPOCHS = 10\n",
        "CHAR_EMBEDDING_DIM = 4\n",
        "CHAR_HIDDEN_DIM = 4"
      ],
      "metadata": {
        "id": "OMsoXAMDO9-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Define Model [5 points]"
      ],
      "metadata": {
        "id": "D8q2EmxmCNfn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7U0wb4OeOsde"
      },
      "outputs": [],
      "source": [
        "class CharPOSTagger(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, char_embedding_dim,\n",
        "                 char_hidden_dim, char_size, vocab_size, tagset_size):\n",
        "        \"\"\"\n",
        "        Define and initialize anything needed for the forward pass.\n",
        "\n",
        "        You are required to create a model with:\n",
        "          an embedding layer for word: that maps words to their embedding space\n",
        "          an embedding layer for character: that maps characters to their embedding space\n",
        "          a character-level LSTM layer: that finds the character-level embedding for a word\n",
        "          a word-level LSTM layer: that takes the concatenated representation per word (word embedding + char-lstm) as input and outputs hidden states\n",
        "          a linear layer: maps from hidden state space to tag space\n",
        "        \"\"\"\n",
        "        super(CharPOSTagger, self).__init__()\n",
        "\n",
        "        ### BEGIN YOUR CODE ###\n",
        "\n",
        "\n",
        "        ### END YOUR CODE ###\n",
        "\n",
        "    def forward(self, sentence, chars):\n",
        "        tag_scores = None\n",
        "        \"\"\"\n",
        "        Implement the forward pass.\n",
        "\n",
        "        Given a tokenized index-mapped sentence and a character sequence as the arguments,\n",
        "        find the corresponding raw scores for tags (without softmax)\n",
        "\n",
        "        returns:: tag_scores (Tensor)\n",
        "        \"\"\"\n",
        "\n",
        "        ### BEGIN YOUR CODE ###\n",
        "\n",
        "\n",
        "        ### END YOUR CODE ###\n",
        "\n",
        "        return tag_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Training [5 points]"
      ],
      "metadata": {
        "id": "IXke-HReCdkq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_char(epoch, model, loss_function, optimizer):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_examples = 0\n",
        "    for sentence, tags in training_data:\n",
        "        \"\"\"\n",
        "        Implement the training method\n",
        "\n",
        "        Hint: you can use the prepare_sequence method for creating index mappings\n",
        "          for sentences. For constructing character input, you may want to left pad\n",
        "          each word to MAX_WORD_LEN first, then use prepare_sequence method to create\n",
        "          index  mappings.\n",
        "        \"\"\"\n",
        "\n",
        "        ### BEGIN YOUR CODE ###\n",
        "\n",
        "        # Zero out the parameter gradients\n",
        "\n",
        "        # Prepare input data (sentences, characters, and gold labels)\n",
        "\n",
        "        # Do forward pass with current batch of input\n",
        "\n",
        "        # Get loss with model predictions and true labels\n",
        "\n",
        "        # Update model parameters\n",
        "\n",
        "        # Increase running total loss and the number of past training samples\n",
        "\n",
        "        ### END YOUR CODE ###\n",
        "\n",
        "    avg_train_loss = train_loss / train_examples\n",
        "    avg_val_loss, val_accuracy = evaluate_char(model, loss_function)\n",
        "\n",
        "    print(f\"Epoch: {epoch}/{EPOCHS}\\tAvg Train Loss: {avg_train_loss:.4f}\\tAvg Val Loss: {avg_val_loss:.4f}\\t Val Accuracy: {val_accuracy:.0f}\")\n",
        "\n",
        "def evaluate_char(model, loss_function):\n",
        "    \"\"\"\n",
        "    returns:: avg_val_loss (float)\n",
        "    returns:: val_accuracy (float)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    val_loss = 0\n",
        "    val_examples = 0\n",
        "    with torch.no_grad():\n",
        "        for sentence, tags in val_data:\n",
        "            \"\"\"\n",
        "            Implement the evaluate method. Find the average validation loss\n",
        "            along with the validation accuracy.\n",
        "\n",
        "            Hint: To find the accuracy, argmax of tag predictions can be used.\n",
        "            \"\"\"\n",
        "\n",
        "            ### BEGIN YOUR CODE ###\n",
        "\n",
        "            # Prepare input data (sentences, characters, and gold labels)\n",
        "\n",
        "            # Do forward pass with current batch of input\n",
        "\n",
        "            # Get loss with model predictions and true labels\n",
        "\n",
        "            # Get the predicted labels\n",
        "\n",
        "            # Get number of correct prediction\n",
        "\n",
        "            # Increase running total loss and the number of past valid samples\n",
        "\n",
        "            ### END YOUR CODE ###\n",
        "    val_accuracy = 100. * correct / val_examples\n",
        "    avg_val_loss = val_loss / val_examples\n",
        "    return avg_val_loss, val_accuracy"
      ],
      "metadata": {
        "id": "ll3IHzmiSxf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-QttCw6Otf-"
      },
      "outputs": [],
      "source": [
        "# Initialize the model, optimizer and the loss function\n",
        "# Hint, you may want to use reduction='sum' in the CrossEntropyLoss function\n",
        "\n",
        "### BEGIN YOUR CODE ###\n",
        "\n",
        "\n",
        "### END YOUR CODE ###\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train_char(epoch, model, loss_function, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xslNYW8EBKMQ"
      },
      "source": [
        "*Hint: Under the default hyperparameter setting, after 5 epochs you should be able to get at least `0.85` accuracy on the validation set.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtsHtaCQIo05"
      },
      "source": [
        "### 3.3 Error analysis [5 points]\n",
        "Write a method to generate predictions for the validation set.\n",
        "Create lists of words, tags predicted by the model and ground truth tags.\n",
        "\n",
        "Then use these lists to carry out error analysis to find the top-10 types of errors made by the model.\n",
        "\n",
        "This part is very similar to part 1.7. You may want to refer to your implementation there."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vUawGsWIo06"
      },
      "outputs": [],
      "source": [
        "def generate_predictions(model, val_data):\n",
        "    \"\"\"\n",
        "    Generate predictions for val_data\n",
        "\n",
        "    Create lists of words, tags predicted by the model and ground truth tags.\n",
        "    Hint: It should look very similar to the evaluate function.\n",
        "\n",
        "    returns:: word_list (str list)\n",
        "    returns:: model_tags (str list)\n",
        "    returns:: gt_tags (str list)\n",
        "    \"\"\"\n",
        "    ### BEGIN YOUR CODE ###\n",
        "\n",
        "\n",
        "    ### END YOUR CODE ###\n",
        "\n",
        "    return word_list, model_tags, gt_tags\n",
        "\n",
        "def error_analysis(word_list, model_tags, gt_tags):\n",
        "    \"\"\"\n",
        "    Carry out error analysis\n",
        "\n",
        "    From those lists collected from the above method, find the\n",
        "    top-10 tuples of (model_tag, ground_truth_tag, frequency, example words)\n",
        "    sorted by frequency\n",
        "\n",
        "    returns: errors (list of tuples)\n",
        "    \"\"\"\n",
        "    ### BEGIN YOUR CODE ###\n",
        "\n",
        "\n",
        "    ### END YOUR CODE ###\n",
        "\n",
        "    return errors\n",
        "\n",
        "word_list, model_tags, gt_tags = generate_predictions(model, val_data)\n",
        "errors = error_analysis(word_list, model_tags, gt_tags)\n",
        "\n",
        "for i in errors[:10]:\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuLl_BSMeovb"
      },
      "source": [
        "**Report your findings here.**  \n",
        "What kinds of errors does the character-level model make as compared to the original model, and why do you think it made them?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Fine-tuned BERT POS Tagger [Extra Credit - 5 points]\n",
        "\n",
        "In the above sections, we trained sequence-based models for POS tagging on a fairly limited dataset of *labeled* part of speech data. However, we can imagine the model is having to both learn the basics of language *and* part of speech tagging simultaneously. Perhaps, we can use a model pre-trained on a much larger corpus of language, and *fine-tune* the model on our specific task.\n",
        "\n",
        "For this, we can use **BERT** (see [*Pre-training of Deep Bidirectional Transformers for Language Understanding*](https://aclanthology.org/N19-1423.pdf) NAACL, 2019). BERT introduces a method of pre-training a transformer encoder and fine-tuning the encoder on downstream tasks, and is extrordinarily infuential in NLP research and engineering (e.g., [`bert-base-uncased`](https://huggingface.co/bert-base-uncased) has 45M downloads per month from Huggingface). The core idea is *transfer learning*, or that pre-training on a self-supervised mask language modeling objective can help with our downstream language task of POS tagging. For a step-by-step introduction to the BERT architecture, please see Jay Almmar's [The Illustrated BERT](http://jalammar.github.io/illustrated-bert/).\n",
        "\n",
        "This section will walk you through the use of the popular **Huggingface Transformers** library (see [*Transformers: State-of-the-Art Natural Language Processing*](https://aclanthology.org/2020.emnlp-demos.6), the [HuggingFace Documentation](https://huggingface.co/transformers/) and [Abhishek Mishra's HF tutorial](https://github.com/abhimishra91/transformers-tutorials)), which is a widely used library for distributing and using transformer models. Luckily, we can think of the HuggingFace library as a wrapper on top of PyTorch, so these sections should look familiar to your work so far.\n",
        "\n",
        "**For this extra credit section, we will use a pre-trained BERT model, and fine-tune it on the POS tagging task.**"
      ],
      "metadata": {
        "id": "VRGyRLbSFT0E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Install `transformers` and download DistilBERT\n",
        "\n",
        "For your fine-tuning code to run a bit faster, we will use a smaller \"distilled\" version of BERT called **DistilBERT** (see [*DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter*](https://arxiv.org/abs/1910.01108)). Fortunately with the `transformers` library, we could swap out the underlying model with no code changes to our dataloaders, architecture or traning setup!"
      ],
      "metadata": {
        "id": "FpxSsAVhFXIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU tokenizers transformers"
      ],
      "metadata": {
        "id": "RZ-T9h2DFZls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If you are interested in what other models are available, you can find a\n",
        "# list of model names here (e.g., roberta-base, bert-base-uncased):\n",
        "# https://huggingface.co/transformers/pretrained_models.html\n",
        "\n",
        "from transformers import DistilBertModel, DistilBertTokenizerFast\n",
        "bert_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
      ],
      "metadata": {
        "id": "1GiJc6b4FZoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's take a look at our DistilBERT architecture\n",
        "bert_model"
      ],
      "metadata": {
        "id": "8izjKB9QFZrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Load the dataset with a PyTorch dataloader\n",
        "\n",
        "Please take a look at the `bert-base-cased` tokenizer on the [Tokenizer Playground](https://huggingface.co/spaces/Xenova/the-tokenizer-playground). Our goal will be to predict the POS of each word, but BERT is trained on sub-word tokens, so we need to segment our dataset such that **only the first token of each word is classified**."
      ],
      "metadata": {
        "id": "9kOLPfCkFZ3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class POSDataset(Dataset):\n",
        "  def __init__(self, data, tokenizer, max_len):\n",
        "    self.data = data\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    \"\"\"\n",
        "    Given an index, return the value in your training data (self.data). Make\n",
        "    sure the full output dict from self.tokenizer is returned, with an additional\n",
        "    value for your labels.\n",
        "\n",
        "    Remember! Your BERT tokenizer will give multiple tokens to words with the\n",
        "    same POS tag. We want the FIRST token be given the tag and all other tokens\n",
        "    to be given -100.\n",
        "\n",
        "    Hint: You may use the prepare_sequence() function from earlier sections\n",
        "    Hint: Our training data is already tokenized, so you may find the `is_split_into_words=True`\n",
        "      and `return_offsets_mapping=True` arguments helpful for getting the token offsets.\n",
        "    Hint: When using the tokenizer, you can also use padding='max_length' for [PAD]\n",
        "      tokens to be added for you.\n",
        "    \"\"\"\n",
        "    encoding = None\n",
        "\n",
        "    ### BEGIN YOUR CODE ###\n",
        "\n",
        "    # Get the sentence and POS tags\n",
        "\n",
        "    # Use the BERT tokenizer (self.tokenizer) to encode the sentence. Make sure to\n",
        "    # truncate the sentence if it is longer than self.max_len, and pad the sentence if it\n",
        "    # is less than self.max_len.\n",
        "\n",
        "    # Create token labels, where the first token of each word is the POS tag, and\n",
        "    # all others are -100.\n",
        "\n",
        "    # Add the token labels back to the tokenized dict\n",
        "\n",
        "    # Make sure both your encoded sentence, labels and attention mask are PyTorch tensors\n",
        "\n",
        "    ### END YOUR CODE ###\n",
        "\n",
        "    return encoding"
      ],
      "metadata": {
        "id": "mrz8yao9Fa2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use your POSDataset class to create a train and test set\n",
        "MAX_LEN = 128\n",
        "\n",
        "# Further split your train data into train/test. You now have train/test/val.\n",
        "train_test_data, split = training_data, int(0.7 * len(training_data))\n",
        "random.shuffle(train_test_data)\n",
        "split_training_data, split_test_data = train_test_data[:split], train_test_data[split:]\n",
        "\n",
        "training_set = POSDataset(split_training_data, tokenizer, MAX_LEN)\n",
        "testing_set = POSDataset(split_test_data, tokenizer, MAX_LEN)\n",
        "validation_set = POSDataset(val_data, tokenizer, MAX_LEN)"
      ],
      "metadata": {
        "id": "cVyxHLOQFa4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print a few values from your Dataloader!\n",
        "print(training_set.__getitem__(0)['input_ids'])\n",
        "print(training_set.__getitem__(0)['labels'])"
      ],
      "metadata": {
        "id": "6sAWlebYFa7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create PyTorch dataloaders from the POSDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "training_loader = DataLoader(training_set, batch_size=64, shuffle=True)\n",
        "testing_loader = DataLoader(testing_set, batch_size=64, shuffle=True)\n",
        "validating_loader = DataLoader(validation_set, batch_size=8, shuffle=True)"
      ],
      "metadata": {
        "id": "_Ptnepq_Fa9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Define your `BertForPOSTagging` Model\n",
        "\n",
        "Now we will modify BERT by extending the `DistilBertModel` class for our task."
      ],
      "metadata": {
        "id": "Aue0XpaOFbK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BertForPOSTagging(DistilBertModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        ### BEGIN YOUR CODE ###\n",
        "\n",
        "\n",
        "\n",
        "        ### END YOUR CODE ###\n",
        "\n",
        "        self.post_init()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        \"\"\"\n",
        "        Forward pass through your model. Returns output logits for each POS\n",
        "        label and the loss (if labels is not None)\n",
        "\n",
        "        Hint: You may use nn.CrossEntropyLoss() to calculate your loss.\n",
        "        \"\"\"\n",
        "        loss, logits = None, None\n",
        "\n",
        "        ### BEGIN YOUR CODE ###\n",
        "\n",
        "\n",
        "\n",
        "        ### END YOUR CODE ###\n",
        "\n",
        "        if loss is not None:\n",
        "          return loss, logits\n",
        "        return logits"
      ],
      "metadata": {
        "id": "JU0xBXEUFb5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForPOSTagging.from_pretrained(\n",
        "    'distilbert-base-uncased',\n",
        "    num_labels=len(tag_to_idx)\n",
        ").to(device)\n",
        "\n",
        "MAX_GRAD_NORM = 10\n",
        "EPOCHS = 5\n",
        "\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-04)"
      ],
      "metadata": {
        "id": "MlQ_tBv8Fb7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4 Training and Evaluation\n",
        "\n",
        "Now we have instantiated our model, please create the train loop!\n",
        "\n",
        "*Hint: If your implementation is correct, you can expect a validation accuracy of `0.88`*"
      ],
      "metadata": {
        "id": "8ehjPIReFcGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DistilBERT will take up a lot of memory (particularly during development)\n",
        "# use this to check the amount of memory you currently have. (Note: you should\n",
        "# be able to fine-tune with ~5 GB of GPU memory)\n",
        "print(f\"Currently allocated GPU memory: {torch.cuda.memory_allocated(device) / 1024**3:.2f} GB / {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "\n",
        "# Hint: use `torch.cuda.empty_cache()` to clear the CUDA cache"
      ],
      "metadata": {
        "id": "-fx-SiOTFdBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "    train_loss = 0\n",
        "    train_examples, train_steps = 0, 0\n",
        "\n",
        "    model.train()\n",
        "    model.zero_grad()\n",
        "\n",
        "    for idx, batch in enumerate(training_loader):\n",
        "        ids = batch['input_ids'].to(device, dtype=torch.long)\n",
        "        mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
        "        labels = batch['labels'].to(device, dtype=torch.long)\n",
        "\n",
        "        ### BEGIN YOUR CODE ###\n",
        "\n",
        "\n",
        "\n",
        "        ### END YOUR CODE ###\n",
        "\n",
        "        train_steps += 1\n",
        "        train_examples += labels.size(0)\n",
        "\n",
        "    avg_train_loss = train_loss / train_steps\n",
        "    avg_val_loss, val_accuracy = evaluate_bert(model)\n",
        "\n",
        "    print(f\"Epoch: {epoch}/{EPOCHS}\\tAvg Train Loss: {avg_train_loss:.4f}\\tAvg Val Loss: {avg_val_loss:.4f}\\t Val Accuracy: {val_accuracy:.0f}\")\n",
        "\n",
        "def evaluate_bert(model):\n",
        "    correct, val_loss, val_examples = 0, 0, 0\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(validating_loader):\n",
        "            \"\"\"\n",
        "            Implement the evaluate method. Find the average validation loss\n",
        "            along with the validation accuracy.\n",
        "\n",
        "            Remember! You have labeled only the first token of each word. Make\n",
        "            sure you only calculate accuracy on values which are not -100.\n",
        "            \"\"\"\n",
        "            ids = batch['input_ids'].to(device, dtype=torch.long)\n",
        "            mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
        "            labels = batch['labels'].to(device, dtype=torch.long)\n",
        "\n",
        "            ### BEGIN YOUR CODE ###\n",
        "\n",
        "\n",
        "            # Compute training accuracy\n",
        "\n",
        "            # Only compute accuracy at active labels\n",
        "\n",
        "            # Get the predicted labels\n",
        "\n",
        "            # Get number of correct predictions\n",
        "\n",
        "            # Increase running total loss and the number of past valid samples\n",
        "\n",
        "\n",
        "            ### END YOUR CODE ###\n",
        "\n",
        "    val_accuracy = 100 * correct / val_examples\n",
        "    avg_val_loss = val_loss / val_examples\n",
        "    return avg_val_loss, val_accuracy"
      ],
      "metadata": {
        "id": "Z51iRKbcFdDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    train(epoch)"
      ],
      "metadata": {
        "id": "WKu-SRnDFdFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.5 Inference\n",
        "\n",
        "Good job! Now we can use our fine-tuned BERT model for POS tagging.\n",
        "\n",
        "In fact, if you have a fine-tuned transformer model (such as in a final project), you could directly upload the model to HuggingFace for others to use (see [this group](https://huggingface.co/QCRI/bert-base-multilingual-cased-pos-english), which fine-tuned on a much larger corpus of POS tags)."
      ],
      "metadata": {
        "id": "59uAtU7eFdQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_prediction(model, sentence):\n",
        "    \"\"\"\n",
        "    Given a sentence, generate a full prediction of POS tags.\n",
        "\n",
        "    In this case, you are given a full sentence (not array of tokens), so you\n",
        "    will need to use your tokenizer differently.\n",
        "\n",
        "    Return your prediction in the format:\n",
        "      [(token 1, POS prediction 1), (token 2, POS prediction 2), ...]\n",
        "\n",
        "    E.g., \"The imperatives that\" => [('the', 'DT'), ('imperative', 'NNS'), ('that', 'WDT')]\n",
        "    \"\"\"\n",
        "    prediction = []\n",
        "\n",
        "    ### BEGIN YOUR CODE ###\n",
        "\n",
        "\n",
        "\n",
        "    ### END YOUR CODE ###\n",
        "\n",
        "    return prediction"
      ],
      "metadata": {
        "id": "YsER2OtOFeEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"The imperatives that can be obeyed by a machine that has no limbs are bound to be of a rather intellectual character.\"\n",
        "print(generate_prediction(model, sentence))"
      ],
      "metadata": {
        "id": "YRt0NvoAFeG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Submit Your Homework\n",
        "This is the end of Project 2. Congratulations!\n",
        "\n",
        "Now, follow the steps below to submit your homework in Gradescope:\n",
        "\n",
        "1. Rename this ipynb file to 'CS4650_p2_GTusername.ipynb'. We recommend ensuring you have removed any extraneous cells & print statements, clearing all outputs, and using the Runtime --> Run all tool to make sure all output is update to date. Additionally, leaving comments in your code to help us understand your operations will assist the teaching staff in grading. It is not a requirement, but is recommended.\n",
        "2. Click on the menu 'File' --> 'Download' --> 'Download .py'.\n",
        "3. Click on the menu 'File' --> 'Download' --> 'Download .ipynb'.\n",
        "4. Download the notebook as a .pdf document. Make sure the output from Parts 1.6 & 2 & 3 are captured so we can see how the loss and accuracy changes while training.\n",
        "5. Upload all 3 files to Gradescope. Double check the files start with `CS4650_p2_*`, capitalization matters.\n",
        "\n"
      ],
      "metadata": {
        "id": "W09rDJA03pcT"
      }
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}