{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fymbc/cs4650/blob/main/PA2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqxpid8J3_xt"
      },
      "source": [
        "# Part of Speech Tagging with LSTM, Fine-tuned BERT\n",
        "**CS 4650 \"Natural Language Processing\" Project 2**  \n",
        "Georgia Tech, Spring 2025 (Instructor: Weicheng Ma)\n",
        "\n",
        "**To start, first make a copy of this notebook to your local drive, so you can edit it.**\n",
        "\n",
        "If you want GPUs (which will improve training speed), you can always change your instance type to GPU by going to Runtime -> Change runtime type -> Hardware accelerator.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_DVctvT4zlV"
      },
      "source": [
        "## 1. Basic POS Tagger  [15 points]\n",
        "\n",
        "In this assignment, we will train LSTM-based POS-taggers, and evaluate their performance. We will use English text from the Wall Street Journal, marked with POS tags such as `NNP` (proper noun) and `DT` (determiner)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3X367eCR3_x0"
      },
      "source": [
        "### 1.1 Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2hVu_ia9Ottg"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100    17  100    17    0     0     37      0 --:--:-- --:--:-- --:--:--    37\n",
            "100    17  100    17    0     0     37      0 --:--:-- --:--:-- --:--:--    37\n",
            "\n",
            "  0 2991k    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "100 2991k  100 2991k    0     0  2016k      0  0:00:01  0:00:01 --:--:--  9.8M\n"
          ]
        }
      ],
      "source": [
        "!curl -L -o train.txt \"https://www.dropbox.com/scl/fi/nqtk53b2ihqzf6hugolms/train.txt?rlkey=y7003b74z7gp06e8qa2gfgd4r&st=37lt5q66&dl=0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DtnGNDoA3_x3"
      },
      "outputs": [],
      "source": [
        "# ===========================================================================\n",
        "# Run some setup code for this notebook. Don't modify anything in this cell.\n",
        "# ===========================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import random\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "\n",
        "# ===========================================================================\n",
        "# A quick note on CUDA functionality (and `.to(model.device)`):\n",
        "# CUDA is a parallel GPU platform produced by NVIDIA and is used by most GPU\n",
        "# libraries in PyTorch. CUDA organizes GPUs into device IDs (i.e., \"cuda:X\" for GPU #X).\n",
        "# \"device\" will tell PyTorch which GPU (or CPU) to place an object in. Since\n",
        "# collab only uses one GPU, we will use 'cuda' as the device if a GPU is available\n",
        "# and the CPU if not. You will run into problems if your tensors are on different devices.\n",
        "# ===========================================================================\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuEywStkc3M5"
      },
      "source": [
        "You can check to make sure a GPU is available using the following code block.\n",
        "\n",
        "\n",
        "```py\n",
        "# If the below message is shown, it means you are using a CPU.\n",
        "/bin/bash: nvidia-smi: command not found\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ounnp0ASc58O"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'nvidia-smi' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwA2y6OR3_yE"
      },
      "source": [
        "### 1.2 Preparing Data\n",
        "\n",
        "`train.txt`: The training data is present in this file. This file contains sequences of words and their respective tags. The data is split into 80% training and 20% development to train the model and tune the hyperparameters, respectively. See `load_tag_data` for details on how to read the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kFpH2P1A3_yG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Data:  7148\n",
            "Val Data:  1788\n",
            "Total tags:  44\n"
          ]
        }
      ],
      "source": [
        "# ===========================================================================\n",
        "# Run some preprocessing code for our dataset. Don't modify anything in this cell.\n",
        "# ===========================================================================\n",
        "\n",
        "def load_tag_data(tag_file):\n",
        "    all_sentences = []\n",
        "    all_tags = []\n",
        "    sent = []\n",
        "    tags = []\n",
        "    with open(tag_file, 'r') as f:\n",
        "        for line in f:\n",
        "            if line.strip() == \"\":\n",
        "                all_sentences.append(sent)\n",
        "                all_tags.append(tags)\n",
        "                sent = []\n",
        "                tags = []\n",
        "            else:\n",
        "                word, tag, _ = line.strip().split()\n",
        "                sent.append(word)\n",
        "                tags.append(tag)\n",
        "    return all_sentences, all_tags\n",
        "\n",
        "train_sentences, train_tags = load_tag_data('train.txt')\n",
        "\n",
        "unique_tags = set([tag for tag_seq in train_tags for tag in tag_seq])\n",
        "\n",
        "# Create train-val split from train data\n",
        "train_val_data = list(zip(train_sentences, train_tags))\n",
        "random.shuffle(train_val_data)\n",
        "split = int(0.8 * len(train_val_data))\n",
        "training_data = train_val_data[:split]\n",
        "val_data = train_val_data[split:]\n",
        "\n",
        "print(\"Train Data: \", len(training_data))\n",
        "print(\"Val Data: \", len(val_data))\n",
        "print(\"Total tags: \", len(unique_tags))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlfliN0J-RzV"
      },
      "source": [
        "### 1.3 Word-to-Index and Tag-to-Index mapping\n",
        "In order to work with text in Tensor format, we need to map each word to an index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "uojEDun83_yP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total tags 44\n",
            "Vocab size 19122\n"
          ]
        }
      ],
      "source": [
        "# ===========================================================================\n",
        "# Don't modify anything in this cell.\n",
        "# ===========================================================================\n",
        "\n",
        "word_to_idx = {}\n",
        "for sent in train_sentences:\n",
        "    for word in sent:\n",
        "        if word not in word_to_idx:\n",
        "            word_to_idx[word] = len(word_to_idx)\n",
        "\n",
        "tag_to_idx = {}\n",
        "for tag in unique_tags:\n",
        "    if tag not in tag_to_idx:\n",
        "        tag_to_idx[tag] = len(tag_to_idx)\n",
        "\n",
        "idx_to_tag = {}\n",
        "for tag in tag_to_idx:\n",
        "    idx_to_tag[tag_to_idx[tag]] = tag\n",
        "\n",
        "print(\"Total tags\", len(tag_to_idx))\n",
        "print(\"Vocab size\", len(word_to_idx))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "H26dqorp3_yX"
      },
      "outputs": [],
      "source": [
        "def prepare_sequence(sent, idx_mapping):\n",
        "    idxs = [idx_mapping[word] for word in sent]\n",
        "    return torch.tensor(idxs, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRnBTCwD3_yc"
      },
      "source": [
        "### 1.4 Set up model\n",
        "We will build and train a Basic POS Tagger which is an LSTM model to tag the parts of speech in a given sentence. Here we define a few default hyperparameters for your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2P5SHabu3_yf"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_DIM = 4\n",
        "HIDDEN_DIM = 8\n",
        "LEARNING_RATE = 0.1\n",
        "LSTM_LAYERS = 1\n",
        "DROPOUT = 0\n",
        "EPOCHS = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkkS4oEb3_yk"
      },
      "source": [
        "### 1.5 Define Model [5 points]\n",
        "\n",
        "The model takes as input a sentence as a tensor in the index space. This sentence is then converted to embedding space where each word maps to its word embedding. The word embeddings is learned as part of the model training process. These word embeddings act as input to the LSTM which produces a representation for each word. Then the representations of words are passed to a Linear layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "aCa30HQb3_ym"
      },
      "outputs": [],
      "source": [
        "class BasicPOSTagger(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "        \"\"\"\n",
        "        Define and initialize anything needed for the forward pass.\n",
        "\n",
        "        You are required to create a model with:\n",
        "          an embedding layer: that maps words to the embedding space\n",
        "          an LSTM layer: that takes word embeddings as input and outputs hidden states\n",
        "          a linear layer: maps from hidden state space to tag space\n",
        "        \"\"\"\n",
        "        super(BasicPOSTagger, self).__init__()\n",
        "\n",
        "        ### BEGIN YOUR CODE ###\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, LSTM_LAYERS, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim, tagset_size)\n",
        "        ### END YOUR CODE ###\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        \"\"\"\n",
        "        Implement the forward pass.\n",
        "\n",
        "        Given a tokenized index-mapped sentence as the argument,\n",
        "        compute the corresponding raw scores for tags (without softmax)\n",
        "\n",
        "        returns:: tag_scores (Tensor)\n",
        "        \"\"\"\n",
        "        tag_scores = None\n",
        "\n",
        "        ### BEGIN YOUR CODE ###\n",
        "        embeddings = self.embedding(sentence)\n",
        "        output, (h_n, c_n) = self.lstm(embeddings)\n",
        "        tag_scores = self.linear(output)\n",
        "\n",
        "        ### END YOUR CODE ###\n",
        "\n",
        "        return tag_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ot9J3MrB3_ys"
      },
      "source": [
        "### 1.6 Training [5 points]\n",
        "\n",
        "We define train and evaluate procedures that allow us to train our model using our created train-val split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BWMGxh4Z3_yv"
      },
      "outputs": [],
      "source": [
        "def train(epoch, model, loss_function, optimizer):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_examples = 0\n",
        "    for sentence, tags in training_data:\n",
        "        \"\"\"\n",
        "        Implement the training method\n",
        "\n",
        "        Hint: you can use the prepare_sequence method for creating index mappings\n",
        "        for sentences. Find the gradient with respect to the loss and update the\n",
        "        model parameters using the optimizer.\n",
        "        \"\"\"\n",
        "\n",
        "        ### BEGIN YOUR CODE ###\n",
        "\n",
        "        # Zero out the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Prepare input data (sentences and gold labels)\n",
        "        input = prepare_sequence(sentence, word_to_idx)\n",
        "        target = prepare_sequence(tags, tag_to_idx)\n",
        "\n",
        "        # Do forward pass with current batch of input\n",
        "        tag_scores = model(input)\n",
        "\n",
        "        # Get loss with model predictions and true labels\n",
        "        loss = loss_function(tag_scores.view(-1, tag_scores.shape[-1]), target.view(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        # Update model parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Increase running total loss and the number of past training samples\n",
        "        train_loss += loss.item()\n",
        "        train_examples += len(sentence)\n",
        "\n",
        "        ### END YOUR CODE ###\n",
        "\n",
        "    avg_train_loss = train_loss / train_examples\n",
        "    avg_val_loss, val_accuracy = evaluate(model, loss_function)\n",
        "\n",
        "    print(f\"Epoch: {epoch}/{EPOCHS}\\tAvg Train Loss: {avg_train_loss:.4f}\\tAvg Val Loss: {avg_val_loss:.4f}\\t Val Accuracy: {val_accuracy:.0f}\")\n",
        "\n",
        "def evaluate(model, loss_function):\n",
        "    \"\"\"\n",
        "    returns:: avg_val_loss (float)\n",
        "    returns:: val_accuracy (float)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    val_loss = 0\n",
        "    val_examples = 0\n",
        "    with torch.no_grad():\n",
        "        for sentence, tags in val_data:\n",
        "            \"\"\"\n",
        "            Implement the evaluate method\n",
        "\n",
        "            Find the average validation loss along with the validation accuracy.\n",
        "            Hint: To find the accuracy, argmax of tag predictions can be used.s\n",
        "            \"\"\"\n",
        "            ### BEGIN YOUR CODE ###\n",
        "\n",
        "            # Prepare input data (sentences and gold labels)\n",
        "            input = prepare_sequence(sentence, word_to_idx)\n",
        "            target = prepare_sequence(tags, tag_to_idx)\n",
        "\n",
        "            # Do forward pass with current batch of input\n",
        "            tag_scores = model(input)\n",
        "\n",
        "            # Get loss with model predictions and true labels\n",
        "            loss = loss_function(tag_scores.view(-1, tag_scores.shape[-1]), target.view(-1))\n",
        "\n",
        "            # Get the predicted labels\n",
        "            _, predicted = torch.max(tag_scores, dim=1)\n",
        "            \n",
        "            # Get number of correct prediction\n",
        "            correct += (predicted.view(-1) == target.view(-1)).sum().item()\n",
        "\n",
        "            # Increase running total loss and the number of past valid samples\n",
        "            val_loss += loss.item()\n",
        "            val_examples += len(sentence)\n",
        "\n",
        "            ### END YOUR CODE ###\n",
        "    val_accuracy = 100. * correct / val_examples\n",
        "    avg_val_loss = val_loss / val_examples\n",
        "    return avg_val_loss, val_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "lsuHjjH1rQeS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1/10\tAvg Train Loss: 0.0734\tAvg Val Loss: 0.0580\t Val Accuracy: 59\n",
            "Epoch: 2/10\tAvg Train Loss: 0.0526\tAvg Val Loss: 0.0489\t Val Accuracy: 65\n",
            "Epoch: 3/10\tAvg Train Loss: 0.0443\tAvg Val Loss: 0.0432\t Val Accuracy: 69\n",
            "Epoch: 4/10\tAvg Train Loss: 0.0384\tAvg Val Loss: 0.0379\t Val Accuracy: 74\n",
            "Epoch: 5/10\tAvg Train Loss: 0.0337\tAvg Val Loss: 0.0348\t Val Accuracy: 78\n",
            "Epoch: 6/10\tAvg Train Loss: 0.0301\tAvg Val Loss: 0.0323\t Val Accuracy: 80\n",
            "Epoch: 7/10\tAvg Train Loss: 0.0274\tAvg Val Loss: 0.0305\t Val Accuracy: 82\n",
            "Epoch: 8/10\tAvg Train Loss: 0.0255\tAvg Val Loss: 0.0296\t Val Accuracy: 83\n",
            "Epoch: 9/10\tAvg Train Loss: 0.0237\tAvg Val Loss: 0.0286\t Val Accuracy: 84\n",
            "Epoch: 10/10\tAvg Train Loss: 0.0223\tAvg Val Loss: 0.0281\t Val Accuracy: 84\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Initialize the model, optimizer and the loss function\n",
        "\"\"\"\n",
        "### BEGIN YOUR CODE ###\n",
        "model = BasicPOSTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_idx), len(tag_to_idx))\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), LEARNING_RATE, momentum=0.8)\n",
        "\n",
        "### END YOUR CODE ###\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train(epoch, model, loss_function, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uK6mT_k8NRvB"
      },
      "source": [
        "*Hint: Under the default hyperparameter setting, after 5 epochs you should be able to get at least `0.75` accuracy on the validation set.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iP64WDReBuDr"
      },
      "source": [
        "### 1.7 Error analysis [5 points]\n",
        "\n",
        "In this step, we will analyze what kind of errors it was making on the validation set.\n",
        "\n",
        "Step 1, write a method to generate predictions from the validation set. For every sentence, get its words, predicted tags (model_tags), and the ground truth tags (gt_tags). To make the next step easier, you may want to concatenate words from all sentences into a very long list, and same for model_tags and gt_tags.\n",
        "\n",
        "\n",
        "Step 2, analyze what kind of errors the model was making. For example, it may frequently label NN as VB. Let's get the top-10 most frequent types of errors, each of their frequency, and some example words. One example is at below. It is interpreted as the model predicts NNP as VBG for 626 times, five random example words are shown.\n",
        "\n",
        "```\n",
        "['VBG', 'NNP', 626, ['Rowe', 'Livermore', 'Parker', 'F-16', 'HEYNOW']]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QgMHr7HCn1x"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('NN', 'NNP', 334, ['Bateman', 'Bryan', 'Galles', 'mature', 'ANC'])\n",
            "('NNS', 'NNP', 299, ['Michigan', 'Barbie', 'Wheels', 'Investor', 'Masius'])\n",
            "('NN', 'JJ', 242, ['ever-narrowing', 'lengthy', 'unrealistic', '60-inch', 'much-beloved'])\n",
            "('NN', 'NNS', 224, ['authorities', 'misrepresentations', 'ounces', 'weeks', 'gases'])\n",
            "('JJ', 'NN', 198, ['headquarters', 'commercial', 'stockpile', 'net', 'many'])\n",
            "('NNS', 'NN', 198, ['humor', 'reflection', 'tandem', 'exception', 're-election'])\n",
            "('NNP', 'JJ', 194, ['California', 'dense', 'solar', '30-year', '30-year'])\n",
            "('JJ', 'NNP', 176, ['Ivy', 'League', 'Telerate', 'Hickman', 'Allenport'])\n",
            "('NNP', 'NN', 176, ['corridor', 'depressant', 'Market', 'chicken', 'Energy'])\n",
            "('NNS', 'VBG', 175, ['buying', 'laughing', 'pushing', 'closing', 'closing'])\n"
          ]
        }
      ],
      "source": [
        "def generate_predictions(model, val_data):\n",
        "    \"\"\"\n",
        "    Generate predictions for val_data\n",
        "\n",
        "    Create lists of words, tags predicted by the model and ground truth tags.\n",
        "    Hint: It should look very similar to the evaluate function.\n",
        "\n",
        "    returns:: word_list (str list)\n",
        "    returns:: model_tags (str list)\n",
        "    returns:: gt_tags (str list)\n",
        "    \"\"\"\n",
        "    ### BEGIN YOUR CODE ###\n",
        "    model.eval()\n",
        "    word_list = []\n",
        "    model_tags = []\n",
        "    gt_tags = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "       for sentence, tags in val_data:\n",
        "          input = prepare_sequence(sentence, word_to_idx)\n",
        "          tag_scores = model(input)\n",
        "\n",
        "          _, predicted = torch.max(tag_scores, dim=1)\n",
        "\n",
        "          word_list.extend(sentence)\n",
        "          model_tags.extend([idx_to_tag[idx.item()] for idx in predicted])\n",
        "          gt_tags.extend(tags)\n",
        "    ### END YOUR CODE ###\n",
        "\n",
        "    return word_list, model_tags, gt_tags\n",
        "\n",
        "def error_analysis(word_list, model_tags, gt_tags):\n",
        "    \"\"\"\"\n",
        "    Carry out error analysis\n",
        "\n",
        "    From those lists collected from the above method, find the\n",
        "    top-10 tuples of (model_tag, ground_truth_tag, frequency, example words)\n",
        "    sorted by frequency\n",
        "\n",
        "    returns: errors (list of tuples)\n",
        "    \"\"\"\n",
        "    ### BEGIN YOUR CODE ###\n",
        "    error_count = {}\n",
        "    error_eg = {}\n",
        "\n",
        "    for i in range(len(word_list)):\n",
        "       pred_tag = model_tags[i]\n",
        "       gt_tag = gt_tags[i]\n",
        "       word = word_list[i]\n",
        "\n",
        "       if pred_tag != gt_tag:\n",
        "        key = (pred_tag, gt_tag)\n",
        "          \n",
        "        if key in error_count:\n",
        "            error_count[key] += 1\n",
        "            if len(error_eg[key]) < 5:\n",
        "                    error_eg[key].append(word)\n",
        "        else:\n",
        "           error_count[key] = 1\n",
        "           error_eg[key] = [word]\n",
        "    \n",
        "    errors = []\n",
        "    for key in error_count:\n",
        "        errors.append((key[0], key[1], error_count[key], error_eg[key]))\n",
        "\n",
        "    errors.sort(key=lambda x: x[2], reverse=True)\n",
        "    ### END YOUR CODE ###\n",
        "\n",
        "    return errors\n",
        "\n",
        "word_list, model_tags, gt_tags = generate_predictions(model, val_data)\n",
        "errors = error_analysis(word_list, model_tags, gt_tags)\n",
        "\n",
        "for i in errors[:10]:\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRNjFRDcD2h7"
      },
      "source": [
        "**Report your findings here.**  \n",
        "What kinds of errors did the model make and why do you think it made them?\n",
        "\n",
        "It frequently misclassifies certain noun forms and adjectives. For instance, proper nouns were misclassified as adjectives, and plural nouns were misclassified as singular nouns. It likely made these errors because many words can have multiple possible POS tags depending on context, and hence it struggles with ambiguous words. Another possible factor is that the model only uses one LSTM layer with no bidirectional processing, therefore not being able to capture sufficient context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLcI9BUZIo04"
      },
      "source": [
        "## 2. Hyper-parameter Tuning [10 points]\n",
        "\n",
        "In order to improve your model performance, try making some modifications on `EMBEDDING_DIM`, `HIDDEN_DIM`, and `LEARNING_RATE`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "RekmpLxzIo04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1/10\tAvg Train Loss: 0.7597\tAvg Val Loss: 0.5029\t Val Accuracy: 86\n",
            "Epoch: 2/10\tAvg Train Loss: 0.3763\tAvg Val Loss: 0.4025\t Val Accuracy: 89\n",
            "Epoch: 3/10\tAvg Train Loss: 0.2441\tAvg Val Loss: 0.3777\t Val Accuracy: 91\n",
            "Epoch: 4/10\tAvg Train Loss: 0.1678\tAvg Val Loss: 0.3787\t Val Accuracy: 92\n",
            "Epoch: 5/10\tAvg Train Loss: 0.1253\tAvg Val Loss: 0.3877\t Val Accuracy: 92\n",
            "Epoch: 6/10\tAvg Train Loss: 0.1001\tAvg Val Loss: 0.4057\t Val Accuracy: 92\n",
            "Epoch: 7/10\tAvg Train Loss: 0.0839\tAvg Val Loss: 0.4117\t Val Accuracy: 92\n",
            "Epoch: 8/10\tAvg Train Loss: 0.0780\tAvg Val Loss: 0.4329\t Val Accuracy: 92\n",
            "Epoch: 9/10\tAvg Train Loss: 0.0756\tAvg Val Loss: 0.4372\t Val Accuracy: 92\n",
            "Epoch: 10/10\tAvg Train Loss: 0.0696\tAvg Val Loss: 0.4500\t Val Accuracy: 92\n"
          ]
        }
      ],
      "source": [
        "YOUR_EMBEDDING_DIM = 32\n",
        "YOUR_HIDDEN_DIM = 64\n",
        "YOUR_LEARNING_RATE = 0.01\n",
        "\n",
        "# Set three hyper-parameters. Initialize the model, optimizer and the loss function\n",
        "# Hint, you may want to use reduction='sum' in the CrossEntropyLoss function\n",
        "\n",
        "### BEGIN YOUR CODE ###\n",
        "model = BasicPOSTagger(YOUR_EMBEDDING_DIM, YOUR_HIDDEN_DIM, len(word_to_idx), len(tag_to_idx))\n",
        "loss_function = nn.CrossEntropyLoss(reduction='sum')\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=YOUR_LEARNING_RATE, momentum=0.9)\n",
        "\n",
        "### END YOUR CODE ###\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train(epoch, model, loss_function, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svXyUssdXZ4r"
      },
      "source": [
        "## 3. Character-level POS Tagger  [15 points]\n",
        "\n",
        "Use the character-level information to augment word embeddings. For example, words that end with -ing or -ly give quite a bit of information about their POS tags. To incorporate this information, run a character-level LSTM on every word to create a character-level representation of the word. Take the last hidden state from the character-level LSTM as the representation and concatenate with the word embedding (as in the `BasicPOSTagger`) to create a new word representation that captures more information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nX4-3AoxSJeY"
      },
      "outputs": [],
      "source": [
        "# Create char to index mapping\n",
        "char_to_idx = {}\n",
        "unique_chars = set()\n",
        "MAX_WORD_LEN = 0\n",
        "\n",
        "for sent in train_sentences:\n",
        "    for word in sent:\n",
        "        for c in word:\n",
        "            unique_chars.add(c)\n",
        "        if len(word) > MAX_WORD_LEN:\n",
        "            MAX_WORD_LEN = len(word)\n",
        "\n",
        "for c in unique_chars:\n",
        "    char_to_idx[c] = len(char_to_idx)\n",
        "char_to_idx[' '] = len(char_to_idx)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xXPsL3nDjAu"
      },
      "source": [
        "### An Aside on Padding\n",
        "\n",
        "#### How to do padding correctly for the characters?\n",
        "\n",
        "\n",
        "Assume we have got a sentence [\"We\", \"love\", \"NLP\"]. You are supposed to first prepend a certain number of blank characters to each of the words in this sentence.\n",
        "\n",
        "How to determine the number of blank characters we need? The calculation of MAX_WORD_LEN is here for help (which we already provide in the starter code). For the given sentence, MAX_WORD_LEN equals 4. Therefore we prepend two blank characters to \"We\", zero blank character to \"love\", and one blank character to \"NLP\". So the resultant padded sentence we get should be [\"  We\", \"love\", \" NLP\"].\n",
        "\n",
        "Then, we feed all characters in [\"  We\", \"love\", \" NLP\"] into a char-embedding layer, and get a tensor of shape (3, 4, char_embedding_dim). To make this tensor's shape proper for the char-level LSTM (nn.LSTM), we need to transpose this tensor, i.e. swap the first and the second dimension. So we get a tensor of shape (4, 3, char_embedding_dim), where 4 corresponds to seq_len and 3 corresponds to batch_size.\n",
        "\n",
        "The last thing you need to do is to obtain the last hidden state from the char-level LSTM, and concatenate it with the word embedding, so that you can get an augmented representation of that word.\n",
        "\n",
        "![padding](https://raw.githubusercontent.com/chaojiang06/chaojiang06.github.io/master/TA/spring2022_CS4650/char_padding.png)\n",
        "  *An illustration for left padding characters*\n",
        "\n",
        "#### Why doing the padding?\n",
        "Someone may ask why we want to do such a kind of padding, instead of directly passing each of the character sequences of each word one by one through an LSTM, to get the last hidden state. The reason is that if you don't do padding, then that means you can only implement this process using \"for loop\". For CharPOSTagger, if you implement it using \"for loop\", the training time would be approximately 150s (GPU) / 250s (CPU) per epoch, while it would be around 30s (GPU) / 150s (CPU) per epoch if you do the padding and feed your data in batches. Therefore, we strongly recommend you learn how to do the padding and transform your data into batches. In fact, those are quite important concepts which you should get yourself familar with, although it might take you some time.\n",
        "\n",
        "#### Why doing *left* padding?\n",
        "Our hypothesis is that the suffixes of English words (e.g., -ly, -ing, etc) are more indicative than prefixes for the part-of-speech (POS). Though LSTM is supposed to be able to handle long sequences, it still lose information along the way and the information closer to the last state (which you use as char-level representations) will be retained better.\n",
        "\n",
        "#### How to understand the dimention change?\n",
        "Assume we have got a sentence with 3 words [\"We\", \"love\", \"NLP\"], and assume the dimension of character embedding is 2, the dimension of word embedding is 4, the dimension of word-level LSTM's hidden layer is 5, the dimension of character-level LSTM's hidden layer is 6.\n",
        "\n",
        "In `BasicPOSTagger`, the dimension change would be:\n",
        "\n",
        "- ------ input ------> $(3\\times 1\\times 4)$\n",
        "- -- word-level LSTM --> $(3\\times 1\\times 5)$\n",
        "- ----- linear layer -----> $(3\\times 1\\times 44)$\n",
        "\n",
        "In `CharPOSTagger`, after padding, character embedding, and swapping, the dimension change would be:\n",
        "\n",
        "- ------ input ------> $($ MAX_WORD_LEN $\\times 3\\times 2)$\n",
        "-  -- character-level LSTM --> $($ MAX_WORD_LEN $\\times 3\\times 6)$\n",
        "- -- Take the last hidden state --> $(3\\times 6)$\n",
        "- -- concatenate with word embedings --> $(3\\times 1\\times 10)$\n",
        "- -- word-level LSTM --> $(3\\times 1\\times 5)$\n",
        "- -- linear layer --> $(3\\times 1\\times 44)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OMsoXAMDO9-6"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_DIM = 4\n",
        "HIDDEN_DIM = 8\n",
        "LEARNING_RATE = 0.1\n",
        "LSTM_LAYERS = 1\n",
        "DROPOUT = 0\n",
        "EPOCHS = 10\n",
        "CHAR_EMBEDDING_DIM = 4\n",
        "CHAR_HIDDEN_DIM = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8q2EmxmCNfn"
      },
      "source": [
        "### 3.1 Define Model [5 points]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7U0wb4OeOsde"
      },
      "outputs": [],
      "source": [
        "class CharPOSTagger(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, char_embedding_dim,\n",
        "                 char_hidden_dim, char_size, vocab_size, tagset_size):\n",
        "        \"\"\"\n",
        "        Define and initialize anything needed for the forward pass.\n",
        "\n",
        "        You are required to create a model with:\n",
        "          an embedding layer for word: that maps words to their embedding space\n",
        "          an embedding layer for character: that maps characters to their embedding space\n",
        "          a character-level LSTM layer: that finds the character-level embedding for a word\n",
        "          a word-level LSTM layer: that takes the concatenated representation per word (word embedding + char-lstm) as input and outputs hidden states\n",
        "          a linear layer: maps from hidden state space to tag space\n",
        "        \"\"\"\n",
        "        super(CharPOSTagger, self).__init__()\n",
        "\n",
        "        ### BEGIN YOUR CODE ###\n",
        "        self.word_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.char_embedding = nn.Embedding(char_size, char_embedding_dim)\n",
        "\n",
        "        self.char_lstm = nn.LSTM(char_embedding_dim, char_hidden_dim, LSTM_LAYERS, batch_first=True)\n",
        "        self.word_lstm = nn.LSTM(embedding_dim + char_hidden_dim, hidden_dim, LSTM_LAYERS, batch_first=True)\n",
        "\n",
        "        self.linear = nn.Linear(hidden_dim, tagset_size)\n",
        "        ### END YOUR CODE ###\n",
        "\n",
        "    def forward(self, sentence, chars):\n",
        "        tag_scores = None\n",
        "        \"\"\"\n",
        "        Implement the forward pass.\n",
        "\n",
        "        Given a tokenized index-mapped sentence and a character sequence as the arguments,\n",
        "        find the corresponding raw scores for tags (without softmax)\n",
        "\n",
        "        returns:: tag_scores (Tensor)\n",
        "        \"\"\"\n",
        "\n",
        "        ### BEGIN YOUR CODE ###\n",
        "        word_embeddings = self.word_embedding(sentence).unsqueeze(0)\n",
        "        batch_size, seq_len, max_word_len = chars.shape\n",
        "        chars = chars.view(-1, max_word_len)\n",
        "        char_embeddings = self.char_embedding(chars)\n",
        "\n",
        "        c_out, (c_hn, c_cn) = self.char_lstm(char_embeddings)\n",
        "        c_hn = c_hn.squeeze(0)\n",
        "\n",
        "        c_hn = c_hn.view(batch_size, seq_len, -1)\n",
        "\n",
        "\n",
        "        combined = torch.cat((word_embeddings, c_hn), dim=2)\n",
        "\n",
        "        w_out, (w_hn, w_cn) = self.word_lstm(combined)\n",
        "\n",
        "        tag_scores = self.linear(w_out)\n",
        "        ### END YOUR CODE ###\n",
        "\n",
        "        return tag_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXke-HReCdkq"
      },
      "source": [
        "### 3.2 Training [5 points]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ll3IHzmiSxf6"
      },
      "outputs": [],
      "source": [
        "def train_char(epoch, model, loss_function, optimizer):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_examples = 0\n",
        "    for sentence, tags in training_data:\n",
        "        \"\"\"\n",
        "        Implement the training method\n",
        "\n",
        "        Hint: you can use the prepare_sequence method for creating index mappings\n",
        "          for sentences. For constructing character input, you may want to left pad\n",
        "          each word to MAX_WORD_LEN first, then use prepare_sequence method to create\n",
        "          index  mappings.\n",
        "        \"\"\"\n",
        "\n",
        "        ### BEGIN YOUR CODE ###\n",
        "\n",
        "        # Zero out the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
        "\n",
        "        # Prepare input data (sentences, characters, and gold labels)\n",
        "        input_words = prepare_sequence(sentence, word_to_idx)\n",
        "\n",
        "        input_chars = [[char_to_idx[c] if c in char_to_idx else char_to_idx[' '] for c in word] for word in sentence]\n",
        "        max_word_len = max(len(word) for word in sentence)\n",
        "        input_chars = [[0] * (max_word_len - len(chars)) + chars for chars in input_chars]\n",
        "        input_chars = torch.tensor(input_chars).unsqueeze(0)\n",
        "        \n",
        "        target = prepare_sequence(tags, tag_to_idx)\n",
        "\n",
        "        # Do forward pass with current batch of input\n",
        "        tag_scores = model(input_words, input_chars)\n",
        "\n",
        "        # Get loss with model predictions and true labels\n",
        "        loss = loss_function(tag_scores.view(-1, tag_scores.shape[-1]), target.view(-1))\n",
        "\n",
        "        # Update model parameters\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Increase running total loss and the number of past training samples\n",
        "        train_loss += loss.item()\n",
        "        train_examples += len(sentence)\n",
        "\n",
        "        ### END YOUR CODE ###\n",
        "\n",
        "    avg_train_loss = train_loss / train_examples\n",
        "    avg_val_loss, val_accuracy = evaluate_char(model, loss_function)\n",
        "\n",
        "    print(f\"Epoch: {epoch}/{EPOCHS}\\tAvg Train Loss: {avg_train_loss:.4f}\\tAvg Val Loss: {avg_val_loss:.4f}\\t Val Accuracy: {val_accuracy:.0f}\")\n",
        "\n",
        "def evaluate_char(model, loss_function):\n",
        "    \"\"\"\n",
        "    returns:: avg_val_loss (float)\n",
        "    returns:: val_accuracy (float)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    val_loss = 0\n",
        "    val_examples = 0\n",
        "    with torch.no_grad():\n",
        "        for sentence, tags in val_data:\n",
        "            \"\"\"\n",
        "            Implement the evaluate method. Find the average validation loss\n",
        "            along with the validation accuracy.\n",
        "\n",
        "            Hint: To find the accuracy, argmax of tag predictions can be used.\n",
        "            \"\"\"\n",
        "\n",
        "            ### BEGIN YOUR CODE ###\n",
        "\n",
        "            # Prepare input data (sentences, characters, and gold labels)\n",
        "            input_words = prepare_sequence(sentence, word_to_idx)\n",
        "\n",
        "            input_chars = [[char_to_idx[c] if c in char_to_idx else char_to_idx[' '] for c in word] for word in sentence]\n",
        "            max_word_len = max(len(word) for word in sentence)\n",
        "            input_chars = [[0] * (max_word_len - len(chars)) + chars for chars in input_chars]\n",
        "            input_chars = torch.tensor(input_chars).unsqueeze(0)\n",
        "\n",
        "            target = prepare_sequence(tags, tag_to_idx)\n",
        "\n",
        "            # Do forward pass with current batch of input\n",
        "            tag_scores = model(input_words, input_chars)\n",
        "\n",
        "            # Get loss with model predictions and true labels\n",
        "            loss = loss_function(tag_scores.view(-1, tag_scores.shape[-1]), target.view(-1))\n",
        "\n",
        "            # Get the predicted labels\n",
        "            _, predicted = torch.max(tag_scores, dim=2)\n",
        "\n",
        "            # Get number of correct prediction\n",
        "            correct += (predicted.view(-1) == target.view(-1)).sum().item()\n",
        "\n",
        "            # Increase running total loss and the number of past valid samples\n",
        "            val_loss += loss.item()\n",
        "            val_examples += len(sentence)\n",
        "\n",
        "            ### END YOUR CODE ###\n",
        "    val_accuracy = 100. * correct / val_examples\n",
        "    avg_val_loss = val_loss / val_examples\n",
        "    return avg_val_loss, val_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "6-QttCw6Otf-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1/10\tAvg Train Loss: 0.0488\tAvg Val Loss: 0.0332\t Val Accuracy: 76\n",
            "Epoch: 2/10\tAvg Train Loss: 0.0305\tAvg Val Loss: 0.0276\t Val Accuracy: 80\n",
            "Epoch: 3/10\tAvg Train Loss: 0.0258\tAvg Val Loss: 0.0243\t Val Accuracy: 83\n",
            "Epoch: 4/10\tAvg Train Loss: 0.0216\tAvg Val Loss: 0.0217\t Val Accuracy: 86\n",
            "Epoch: 5/10\tAvg Train Loss: 0.0189\tAvg Val Loss: 0.0201\t Val Accuracy: 87\n",
            "Epoch: 6/10\tAvg Train Loss: 0.0163\tAvg Val Loss: 0.0184\t Val Accuracy: 88\n",
            "Epoch: 7/10\tAvg Train Loss: 0.0195\tAvg Val Loss: 0.0215\t Val Accuracy: 87\n",
            "Epoch: 8/10\tAvg Train Loss: 0.0178\tAvg Val Loss: 0.0204\t Val Accuracy: 88\n",
            "Epoch: 9/10\tAvg Train Loss: 0.0161\tAvg Val Loss: 0.0220\t Val Accuracy: 87\n",
            "Epoch: 10/10\tAvg Train Loss: 0.0154\tAvg Val Loss: 0.0205\t Val Accuracy: 89\n"
          ]
        }
      ],
      "source": [
        "# Initialize the model, optimizer and the loss function\n",
        "# Hint, you may want to use reduction='sum' in the CrossEntropyLoss function\n",
        "\n",
        "### BEGIN YOUR CODE ###\n",
        "model = CharPOSTagger(EMBEDDING_DIM, HIDDEN_DIM, CHAR_EMBEDDING_DIM, CHAR_HIDDEN_DIM, len(char_to_idx), len(word_to_idx), len(tag_to_idx))\n",
        "loss_function = nn.CrossEntropyLoss(reduction='mean')\n",
        "optimizer = torch.optim.SGD(model.parameters(), LEARNING_RATE, momentum=0.9)\n",
        "### END YOUR CODE ###\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train_char(epoch, model, loss_function, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xslNYW8EBKMQ"
      },
      "source": [
        "*Hint: Under the default hyperparameter setting, after 5 epochs you should be able to get at least `0.85` accuracy on the validation set.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtsHtaCQIo05"
      },
      "source": [
        "### 3.3 Error analysis [5 points]\n",
        "Write a method to generate predictions for the validation set.\n",
        "Create lists of words, tags predicted by the model and ground truth tags.\n",
        "\n",
        "Then use these lists to carry out error analysis to find the top-10 types of errors made by the model.\n",
        "\n",
        "This part is very similar to part 1.7. You may want to refer to your implementation there."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "0vUawGsWIo06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('NNP', 'NN', 388, ['hotdog', 'humor', 'abortion', 'abortion', 'procedure'])\n",
            "('NNP', 'VB', 332, ['extract', 'haul', 'Get', 'write', 'become'])\n",
            "('NN', 'DT', 326, ['The', 'The', 'The', 'The', 'The'])\n",
            "('NNP', 'JJ', 292, ['slick-talking', 'snake-oil', 'Initial', 'ever-narrowing', 'bargain-basement'])\n",
            "('NN', 'JJ', 291, ['gullible', 'greedy', 'unrealistic', '60-inch', 'deflationary'])\n",
            "('VBZ', 'NNS', 267, ['buddies', 'portions', 'medicines', 'pleas', 'loopholes'])\n",
            "('NN', 'VBG', 243, ['declining', 'buying', 'collapsing', 'contemplating', 'pushing'])\n",
            "('JJ', 'VB', 243, ['raise', 'boost', 'make', 'have', 'enact'])\n",
            "('VBD', 'VBN', 241, ['alleged', 'made', 'adjusted', 'ended', 'made'])\n",
            "('JJ', 'CD', 232, ['zero', '1992', '334,000', '52', '3.75'])\n"
          ]
        }
      ],
      "source": [
        "def generate_predictions(model, val_data):\n",
        "    \"\"\"\n",
        "    Generate predictions for val_data\n",
        "\n",
        "    Create lists of words, tags predicted by the model and ground truth tags.\n",
        "    Hint: It should look very similar to the evaluate function.\n",
        "\n",
        "    returns:: word_list (str list)\n",
        "    returns:: model_tags (str list)\n",
        "    returns:: gt_tags (str list)\n",
        "    \"\"\"\n",
        "    ### BEGIN YOUR CODE ###\n",
        "    model.eval()\n",
        "    word_list = []\n",
        "    model_tags = []\n",
        "    gt_tags = []\n",
        "    with torch.no_grad():\n",
        "        for sentence, tags in val_data:\n",
        "            input = prepare_sequence(sentence, word_to_idx)\n",
        "            \n",
        "            input_chars = [[char_to_idx[c] if c in char_to_idx else char_to_idx[' '] for c in word] for word in sentence]\n",
        "            max_word_len = max(len(word) for word in sentence)\n",
        "            input_chars = [[0] * (max_word_len - len(chars)) + chars for chars in input_chars]\n",
        "            input_chars = torch.tensor(input_chars).unsqueeze(0)\n",
        "            tag_scores = model(input, input_chars)\n",
        "            \n",
        "            _, predicted = torch.max(tag_scores, dim=2)\n",
        "            predicted = predicted.view(-1)\n",
        "\n",
        "            word_list.extend(sentence)\n",
        "            model_tags.extend([idx_to_tag[idx.item()] for idx in predicted])\n",
        "            gt_tags.extend(tags)\n",
        "\n",
        "\n",
        "    ### END YOUR CODE ###\n",
        "\n",
        "    return word_list, model_tags, gt_tags\n",
        "\n",
        "def error_analysis(word_list, model_tags, gt_tags):\n",
        "    \"\"\"\n",
        "    Carry out error analysis\n",
        "\n",
        "    From those lists collected from the above method, find the\n",
        "    top-10 tuples of (model_tag, ground_truth_tag, frequency, example words)\n",
        "    sorted by frequency\n",
        "\n",
        "    returns: errors (list of tuples)\n",
        "    \"\"\"\n",
        "    ### BEGIN YOUR CODE ###\n",
        "    error_count = {}\n",
        "    error_eg = {}\n",
        "\n",
        "    for i in range(len(word_list)):\n",
        "        pred_tag = model_tags[i]\n",
        "        gt_tag = gt_tags[i]\n",
        "        word = word_list[i]\n",
        "\n",
        "        if pred_tag != gt_tag:\n",
        "            key = (pred_tag, gt_tag)\n",
        "            \n",
        "            if key in error_count:\n",
        "                error_count[key] += 1\n",
        "                if len(error_eg[key]) < 5:\n",
        "                    error_eg[key].append(word)\n",
        "            else:\n",
        "                error_count[key] = 1\n",
        "                error_eg[key] = [word]\n",
        "    \n",
        "    errors = []\n",
        "    for key in error_count:\n",
        "        errors.append((key[0], key[1], error_count[key], error_eg[key]))\n",
        "\n",
        "    errors.sort(key=lambda x: x[2], reverse=True)\n",
        "    ### END YOUR CODE ###\n",
        "\n",
        "    return errors\n",
        "\n",
        "word_list, model_tags, gt_tags = generate_predictions(model, val_data)\n",
        "errors = error_analysis(word_list, model_tags, gt_tags)\n",
        "\n",
        "for i in errors[:10]:\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuLl_BSMeovb"
      },
      "source": [
        "**Report your findings here.**  \n",
        "What kinds of errors does the character-level model make as compared to the original model, and why do you think it made them?\n",
        "\n",
        "The character-level model frequently misclassifies common nouns (NN) as proper nouns (NNP) and vice versa. This suggests that it may rely on capitalization patterns but struggles with unusual or context-dependent proper nouns. The original model on the other hand misclassifies named entities possibly due to limited exposure to specific proper names in the training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRGyRLbSFT0E"
      },
      "source": [
        "## 4. Fine-tuned BERT POS Tagger [Extra Credit - 5 points]\n",
        "\n",
        "In the above sections, we trained sequence-based models for POS tagging on a fairly limited dataset of *labeled* part of speech data. However, we can imagine the model is having to both learn the basics of language *and* part of speech tagging simultaneously. Perhaps, we can use a model pre-trained on a much larger corpus of language, and *fine-tune* the model on our specific task.\n",
        "\n",
        "For this, we can use **BERT** (see [*Pre-training of Deep Bidirectional Transformers for Language Understanding*](https://aclanthology.org/N19-1423.pdf) NAACL, 2019). BERT introduces a method of pre-training a transformer encoder and fine-tuning the encoder on downstream tasks, and is extrordinarily infuential in NLP research and engineering (e.g., [`bert-base-uncased`](https://huggingface.co/bert-base-uncased) has 45M downloads per month from Huggingface). The core idea is *transfer learning*, or that pre-training on a self-supervised mask language modeling objective can help with our downstream language task of POS tagging. For a step-by-step introduction to the BERT architecture, please see Jay Almmar's [The Illustrated BERT](http://jalammar.github.io/illustrated-bert/).\n",
        "\n",
        "This section will walk you through the use of the popular **Huggingface Transformers** library (see [*Transformers: State-of-the-Art Natural Language Processing*](https://aclanthology.org/2020.emnlp-demos.6), the [HuggingFace Documentation](https://huggingface.co/transformers/) and [Abhishek Mishra's HF tutorial](https://github.com/abhimishra91/transformers-tutorials)), which is a widely used library for distributing and using transformer models. Luckily, we can think of the HuggingFace library as a wrapper on top of PyTorch, so these sections should look familiar to your work so far.\n",
        "\n",
        "**For this extra credit section, we will use a pre-trained BERT model, and fine-tune it on the POS tagging task.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpxSsAVhFXIv"
      },
      "source": [
        "### 4.1 Install `transformers` and download DistilBERT\n",
        "\n",
        "For your fine-tuning code to run a bit faster, we will use a smaller \"distilled\" version of BERT called **DistilBERT** (see [*DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter*](https://arxiv.org/abs/1910.01108)). Fortunately with the `transformers` library, we could swap out the underlying model with no code changes to our dataloaders, architecture or traning setup!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "RZ-T9h2DFZls"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: Failed to write executable - trying to use .deleteme logic\n",
            "ERROR: Could not install packages due to an OSError: [WinError 2] The system cannot find the file specified: 'C:\\\\Python312\\\\Scripts\\\\normalizer.exe' -> 'C:\\\\Python312\\\\Scripts\\\\normalizer.exe.deleteme'\n",
            "\n",
            "\n",
            "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install -qU tokenizers transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "1GiJc6b4FZoZ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "c:\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Wei Xuan\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        }
      ],
      "source": [
        "# If you are interested in what other models are available, you can find a\n",
        "# list of model names here (e.g., roberta-base, bert-base-uncased):\n",
        "# https://huggingface.co/transformers/pretrained_models.html\n",
        "\n",
        "from transformers import DistilBertModel, DistilBertTokenizerFast\n",
        "bert_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "8izjKB9QFZrL"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DistilBertModel(\n",
              "  (embeddings): Embeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (transformer): Transformer(\n",
              "    (layer): ModuleList(\n",
              "      (0-5): 6 x TransformerBlock(\n",
              "        (attention): DistilBertSdpaAttention(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (ffn): FFN(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (activation): GELUActivation()\n",
              "        )\n",
              "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's take a look at our DistilBERT architecture\n",
        "bert_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kOLPfCkFZ3t"
      },
      "source": [
        "### 4.2 Load the dataset with a PyTorch dataloader\n",
        "\n",
        "Please take a look at the `bert-base-cased` tokenizer on the [Tokenizer Playground](https://huggingface.co/spaces/Xenova/the-tokenizer-playground). Our goal will be to predict the POS of each word, but BERT is trained on sub-word tokens, so we need to segment our dataset such that **only the first token of each word is classified**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "mrz8yao9Fa2b"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class POSDataset(Dataset):\n",
        "  def __init__(self, data, tokenizer, max_len):\n",
        "    self.data = data\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    \"\"\"\n",
        "    Given an index, return the value in your training data (self.data). Make\n",
        "    sure the full output dict from self.tokenizer is returned, with an additional\n",
        "    value for your labels.\n",
        "\n",
        "    Remember! Your BERT tokenizer will give multiple tokens to words with the\n",
        "    same POS tag. We want the FIRST token be given the tag and all other tokens\n",
        "    to be given -100.\n",
        "\n",
        "    Hint: You may use the prepare_sequence() function from earlier sections\n",
        "    Hint: Our training data is already tokenized, so you may find the `is_split_into_words=True`\n",
        "      and `return_offsets_mapping=True` arguments helpful for getting the token offsets.\n",
        "    Hint: When using the tokenizer, you can also use padding='max_length' for [PAD]\n",
        "      tokens to be added for you.\n",
        "    \"\"\"\n",
        "    encoding = None\n",
        "\n",
        "    ### BEGIN YOUR CODE ###\n",
        "\n",
        "    # Get the sentence and POS tags\n",
        "    sentence, tags = self.data[index]\n",
        "    tag_indices = [tag_to_idx[tag] for tag in tags]\n",
        "\n",
        "    # Use the BERT tokenizer (self.tokenizer) to encode the sentence. Make sure to\n",
        "    # truncate the sentence if it is longer than self.max_len, and pad the sentence if it\n",
        "    # is less than self.max_len.\n",
        "    encoding = self.tokenizer(\n",
        "            sentence,\n",
        "            is_split_into_words=True,\n",
        "            return_offsets_mapping=True,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "    \n",
        "    input_ids = encoding[\"input_ids\"].squeeze(0)\n",
        "    attention_mask = encoding[\"attention_mask\"].squeeze(0)\n",
        "    offset_mapping = encoding[\"offset_mapping\"].squeeze(0)\n",
        "\n",
        "    # Create token labels, where the first token of each word is the POS tag, and\n",
        "    # all others are -100.\n",
        "    labels = torch.full((self.max_len,), -100, dtype=torch.long)\n",
        "\n",
        "    # Add the token labels back to the tokenized dict\n",
        "    word_idx = -1\n",
        "    for i, (start, end) in enumerate(offset_mapping):\n",
        "        if start == 0 and end != 0:\n",
        "            word_idx += 1\n",
        "            if word_idx < len(tag_indices):\n",
        "                labels[i] = tag_indices[word_idx]\n",
        "\n",
        "    # Make sure both your encoded sentence, labels and attention mask are PyTorch tensors\n",
        "    encoding[\"labels\"] = labels\n",
        "    encoding[\"input_ids\"] = input_ids\n",
        "    encoding[\"attention_mask\"] = attention_mask\n",
        "\n",
        "    ### END YOUR CODE ###\n",
        "\n",
        "    return encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "cVyxHLOQFa4r"
      },
      "outputs": [],
      "source": [
        "# Use your POSDataset class to create a train and test set\n",
        "MAX_LEN = 128\n",
        "\n",
        "# Further split your train data into train/test. You now have train/test/val.\n",
        "train_test_data, split = training_data, int(0.7 * len(training_data))\n",
        "random.shuffle(train_test_data)\n",
        "split_training_data, split_test_data = train_test_data[:split], train_test_data[split:]\n",
        "\n",
        "training_set = POSDataset(split_training_data, tokenizer, MAX_LEN)\n",
        "testing_set = POSDataset(split_test_data, tokenizer, MAX_LEN)\n",
        "validation_set = POSDataset(val_data, tokenizer, MAX_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "6sAWlebYFa7X"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([  101,  2021,  1996,  2047,  4696,  2515,  2025,  5672,  2030,  5547,\n",
            "         8256,  1005,  1055,  5041,  5762, 18394,  1012,   102,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0])\n",
            "tensor([-100,   21,   13,   19,    6,   14,    8,   25,   21,   25,   26,   15,\n",
            "        -100,   19,   19,   40,   12, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100])\n"
          ]
        }
      ],
      "source": [
        "# Print a few values from your Dataloader!\n",
        "print(training_set.__getitem__(0)['input_ids'])\n",
        "print(training_set.__getitem__(0)['labels'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "_Ptnepq_Fa9T"
      },
      "outputs": [],
      "source": [
        "# Create PyTorch dataloaders from the POSDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "training_loader = DataLoader(training_set, batch_size=64, shuffle=True)\n",
        "testing_loader = DataLoader(testing_set, batch_size=64, shuffle=True)\n",
        "validating_loader = DataLoader(validation_set, batch_size=8, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aue0XpaOFbK7"
      },
      "source": [
        "### 4.3 Define your `BertForPOSTagging` Model\n",
        "\n",
        "Now we will modify BERT by extending the `DistilBertModel` class for our task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JU0xBXEUFb5U"
      },
      "outputs": [],
      "source": [
        "class BertForPOSTagging(DistilBertModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        ### BEGIN YOUR CODE ###\n",
        "        ### END YOUR CODE ###\n",
        "\n",
        "        self.post_init()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        \"\"\"\n",
        "        Forward pass through your model. Returns output logits for each POS\n",
        "        label and the loss (if labels is not None)\n",
        "\n",
        "        Hint: You may use nn.CrossEntropyLoss() to calculate your loss.\n",
        "        \"\"\"\n",
        "        loss, logits = None, None\n",
        "\n",
        "        ### BEGIN YOUR CODE ###\n",
        "\n",
        "\n",
        "\n",
        "        ### END YOUR CODE ###\n",
        "\n",
        "        if loss is not None:\n",
        "          return loss, logits\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlQ_tBv8Fb7h"
      },
      "outputs": [],
      "source": [
        "model = BertForPOSTagging.from_pretrained(\n",
        "    'distilbert-base-uncased',\n",
        "    num_labels=len(tag_to_idx)\n",
        ").to(device)\n",
        "\n",
        "MAX_GRAD_NORM = 10\n",
        "EPOCHS = 5\n",
        "\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-04)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ehjPIReFcGg"
      },
      "source": [
        "### 4.4 Training and Evaluation\n",
        "\n",
        "Now we have instantiated our model, please create the train loop!\n",
        "\n",
        "*Hint: If your implementation is correct, you can expect a validation accuracy of `0.88`*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "-fx-SiOTFdBL"
      },
      "outputs": [
        {
          "ename": "AssertionError",
          "evalue": "Torch not compiled with CUDA enabled",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[41], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# DistilBERT will take up a lot of memory (particularly during development)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# use this to check the amount of memory you currently have. (Note: you should\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# be able to fine-tune with ~5 GB of GPU memory)\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrently allocated GPU memory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmemory_allocated(device)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m3\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m GB / \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtotal_memory\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m3\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m GB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Hint: use `torch.cuda.empty_cache()` to clear the CUDA cache\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\cuda\\__init__.py:523\u001b[0m, in \u001b[0;36mget_device_properties\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_device_properties\u001b[39m(device: Optional[_device_t] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _CudaDeviceProperties:\n\u001b[0;32m    512\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get the properties of a device.\u001b[39;00m\n\u001b[0;32m    513\u001b[0m \n\u001b[0;32m    514\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;124;03m        _CudaDeviceProperties: the properties of the device\u001b[39;00m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 523\u001b[0m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# will define _get_device_properties\u001b[39;00m\n\u001b[0;32m    524\u001b[0m     device \u001b[38;5;241m=\u001b[39m _get_device_index(device, optional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    525\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count():\n",
            "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\cuda\\__init__.py:310\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    313\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    314\u001b[0m     )\n",
            "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
          ]
        }
      ],
      "source": [
        "# DistilBERT will take up a lot of memory (particularly during development)\n",
        "# use this to check the amount of memory you currently have. (Note: you should\n",
        "# be able to fine-tune with ~5 GB of GPU memory)\n",
        "print(f\"Currently allocated GPU memory: {torch.cuda.memory_allocated(device) / 1024**3:.2f} GB / {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "\n",
        "# Hint: use `torch.cuda.empty_cache()` to clear the CUDA cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z51iRKbcFdDg"
      },
      "outputs": [],
      "source": [
        "def train(epoch):\n",
        "    train_loss = 0\n",
        "    train_examples, train_steps = 0, 0\n",
        "\n",
        "    model.train()\n",
        "    model.zero_grad()\n",
        "\n",
        "    for idx, batch in enumerate(training_loader):\n",
        "        ids = batch['input_ids'].to(device, dtype=torch.long)\n",
        "        mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
        "        labels = batch['labels'].to(device, dtype=torch.long)\n",
        "\n",
        "        ### BEGIN YOUR CODE ###\n",
        "\n",
        "\n",
        "\n",
        "        ### END YOUR CODE ###\n",
        "\n",
        "        train_steps += 1\n",
        "        train_examples += labels.size(0)\n",
        "\n",
        "    avg_train_loss = train_loss / train_steps\n",
        "    avg_val_loss, val_accuracy = evaluate_bert(model)\n",
        "\n",
        "    print(f\"Epoch: {epoch}/{EPOCHS}\\tAvg Train Loss: {avg_train_loss:.4f}\\tAvg Val Loss: {avg_val_loss:.4f}\\t Val Accuracy: {val_accuracy:.0f}\")\n",
        "\n",
        "def evaluate_bert(model):\n",
        "    correct, val_loss, val_examples = 0, 0, 0\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(validating_loader):\n",
        "            \"\"\"\n",
        "            Implement the evaluate method. Find the average validation loss\n",
        "            along with the validation accuracy.\n",
        "\n",
        "            Remember! You have labeled only the first token of each word. Make\n",
        "            sure you only calculate accuracy on values which are not -100.\n",
        "            \"\"\"\n",
        "            ids = batch['input_ids'].to(device, dtype=torch.long)\n",
        "            mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
        "            labels = batch['labels'].to(device, dtype=torch.long)\n",
        "\n",
        "            ### BEGIN YOUR CODE ###\n",
        "\n",
        "\n",
        "            # Compute training accuracy\n",
        "\n",
        "            # Only compute accuracy at active labels\n",
        "\n",
        "            # Get the predicted labels\n",
        "\n",
        "            # Get number of correct predictions\n",
        "\n",
        "            # Increase running total loss and the number of past valid samples\n",
        "\n",
        "\n",
        "            ### END YOUR CODE ###\n",
        "\n",
        "    val_accuracy = 100 * correct / val_examples\n",
        "    avg_val_loss = val_loss / val_examples\n",
        "    return avg_val_loss, val_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKu-SRnDFdFv"
      },
      "outputs": [],
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    train(epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59uAtU7eFdQD"
      },
      "source": [
        "### 4.5 Inference\n",
        "\n",
        "Good job! Now we can use our fine-tuned BERT model for POS tagging.\n",
        "\n",
        "In fact, if you have a fine-tuned transformer model (such as in a final project), you could directly upload the model to HuggingFace for others to use (see [this group](https://huggingface.co/QCRI/bert-base-multilingual-cased-pos-english), which fine-tuned on a much larger corpus of POS tags)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsER2OtOFeEr"
      },
      "outputs": [],
      "source": [
        "def generate_prediction(model, sentence):\n",
        "    \"\"\"\n",
        "    Given a sentence, generate a full prediction of POS tags.\n",
        "\n",
        "    In this case, you are given a full sentence (not array of tokens), so you\n",
        "    will need to use your tokenizer differently.\n",
        "\n",
        "    Return your prediction in the format:\n",
        "      [(token 1, POS prediction 1), (token 2, POS prediction 2), ...]\n",
        "\n",
        "    E.g., \"The imperatives that\" => [('the', 'DT'), ('imperative', 'NNS'), ('that', 'WDT')]\n",
        "    \"\"\"\n",
        "    prediction = []\n",
        "\n",
        "    ### BEGIN YOUR CODE ###\n",
        "\n",
        "\n",
        "\n",
        "    ### END YOUR CODE ###\n",
        "\n",
        "    return prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRt0NvoAFeG1"
      },
      "outputs": [],
      "source": [
        "sentence = \"The imperatives that can be obeyed by a machine that has no limbs are bound to be of a rather intellectual character.\"\n",
        "print(generate_prediction(model, sentence))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W09rDJA03pcT"
      },
      "source": [
        "## 5. Submit Your Homework\n",
        "This is the end of Project 2. Congratulations!\n",
        "\n",
        "Now, follow the steps below to submit your homework in Gradescope:\n",
        "\n",
        "1. Rename this ipynb file to 'CS4650_p2_GTusername.ipynb'. We recommend ensuring you have removed any extraneous cells & print statements, clearing all outputs, and using the Runtime --> Run all tool to make sure all output is update to date. Additionally, leaving comments in your code to help us understand your operations will assist the teaching staff in grading. It is not a requirement, but is recommended.\n",
        "2. Click on the menu 'File' --> 'Download' --> 'Download .py'.\n",
        "3. Click on the menu 'File' --> 'Download' --> 'Download .ipynb'.\n",
        "4. Download the notebook as a .pdf document. Make sure the output from Parts 1.6 & 2 & 3 are captured so we can see how the loss and accuracy changes while training.\n",
        "5. Upload all 3 files to Gradescope. Double check the files start with `CS4650_p2_*`, capitalization matters.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
