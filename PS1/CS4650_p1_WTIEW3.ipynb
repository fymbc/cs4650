{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fymbc/cs4650/blob/main/PS1\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JnnwupedFEIV",
      "metadata": {
        "id": "JnnwupedFEIV"
      },
      "source": [
        "# Headline Classification with Neural BOW, LSTM\n",
        "**CS 4650 \"Natural Language Processing\" Project 1**  \n",
        "Georgia Tech, Spring 2025 (Instructor: Weicheng Ma)\n",
        "\n",
        "Welcome to the first full programming project for CS 4650! **To start, first make a copy of this notebook to your local drive, so you can edit it.**\n",
        "\n",
        "If you want GPUs (which will improve training times), you can always change your instance type by going to Runtime -> Change runtime type -> Hardware accelerator.\n",
        "\n",
        "**In this project, we will be using PyTorch.** If you are new to PyTorch, or simply want a refresher, we recommend you start by looking through these [Introduction to PyTorch](https://sites.cc.gatech.edu/classes/AY2021/cs7650_fall/slides/Introduction_to_PyTorch.pdf) slides and this interactive [PyTorch basics notebook](http://bit.ly/pytorchbasics). Additionally, this [text sentiment](http://bit.ly/pytorchexample) notebook will provide some insight into working with PyTorch with a specific NLP task."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZRdPqeMMFEIY",
      "metadata": {
        "id": "ZRdPqeMMFEIY"
      },
      "source": [
        "## 1. Load and preprocess data [10 points]\n",
        "This project will be modeling a *classification task* for headlines from [The Onion](https://www.theonion.com), a satirical news website. Our dataset contains headlines and whether they belong to The Onion or CNN. Given a headline, we want to predict whether it is Onion or not.\n",
        "\n",
        "The following cell loads, pre-processes and tokenizes our OnionOrNot dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "id": "YmV_uknBJA-o",
      "metadata": {
        "id": "YmV_uknBJA-o"
      },
      "outputs": [],
      "source": [
        "!curl -so OnionOrNot.csv https://raw.githubusercontent.com/lukefeilberg/onion/master/OnionOrNot.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "id": "L3DkMDu7FEIZ",
      "metadata": {
        "id": "L3DkMDu7FEIZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.12.0 (tags/v3.12.0:0fb18b0, Oct  2 2023, 13:03:39) [MSC v.1935 64 bit (AMD64)]\n"
          ]
        }
      ],
      "source": [
        "# ===========================================================================\n",
        "# Run some setup code for this notebook. Don't modify anything in this cell.\n",
        "# ===========================================================================\n",
        "\n",
        "import torch\n",
        "import random, sys\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "\n",
        "# ===========================================================================\n",
        "# A quick note on CUDA functionality (and `.to(model.device)`):\n",
        "# CUDA is a parallel GPU platform produced by NVIDIA and is used by most GPU\n",
        "# libraries in PyTorch. CUDA organizes GPUs into device IDs (i.e., \"cuda:X\" for GPU #X).\n",
        "# \"device\" will tell PyTorch which GPU (or CPU) to place an object in. Since\n",
        "# collab only uses one GPU, we will use 'cuda' as the device if a GPU is available\n",
        "# and the CPU if not. You will run into problems if your tensors are on different devices.\n",
        "# ===========================================================================\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Check what version of Python is running\n",
        "print(sys.version)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0Fulh0MZ8y8b",
      "metadata": {
        "id": "0Fulh0MZ8y8b"
      },
      "source": [
        "### 1.1 Dataset preprocessing functions\n",
        "The following cell define some methods to clean the dataset, but feel free to take a look to see some of the operations it's doing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "id": "ctNnE1Ui8oKw",
      "metadata": {
        "id": "ctNnE1Ui8oKw"
      },
      "outputs": [],
      "source": [
        "# ===========================================================================\n",
        "# Run some preprocessing code for our dataset. Don't modify anything in this cell.\n",
        "# This code was adapted from fast-bert.\n",
        "# ===========================================================================\n",
        "\n",
        "import re\n",
        "import html\n",
        "\n",
        "def spec_add_spaces(t: str) -> str:\n",
        "    \"Add spaces around / and # in `t`. \\n\"\n",
        "    return re.sub(r\"([/#\\n])\", r\" \\1 \", t)\n",
        "\n",
        "def rm_useless_spaces(t: str) -> str:\n",
        "    \"Remove multiple spaces in `t`.\"\n",
        "    return re.sub(\" {2,}\", \" \", t)\n",
        "\n",
        "def replace_multi_newline(t: str) -> str:\n",
        "    return re.sub(r\"(\\n(\\s)*){2,}\", \"\\n\", t)\n",
        "\n",
        "def fix_html(x: str) -> str:\n",
        "    \"List of replacements from html strings in `x`.\"\n",
        "    re1 = re.compile(r\"  +\")\n",
        "    x = (\n",
        "        x.replace(\"#39;\", \"'\")\n",
        "        .replace(\"amp;\", \"&\")\n",
        "        .replace(\"#146;\", \"'\")\n",
        "        .replace(\"nbsp;\", \" \")\n",
        "        .replace(\"#36;\", \"$\")\n",
        "        .replace(\"\\\\n\", \"\\n\")\n",
        "        .replace(\"quot;\", \"'\")\n",
        "        .replace(\"<br />\", \"\\n\")\n",
        "        .replace('\\\\\"', '\"')\n",
        "        .replace(\" @.@ \", \".\")\n",
        "        .replace(\" @-@ \", \"-\")\n",
        "        .replace(\" @,@ \", \",\")\n",
        "        .replace(\"\\\\\", \" \\\\ \")\n",
        "    )\n",
        "    return re1.sub(\" \", html.unescape(x))\n",
        "\n",
        "def clean_text(input_text):\n",
        "    text = fix_html(input_text)\n",
        "    text = replace_multi_newline(text)\n",
        "    text = spec_add_spaces(text)\n",
        "    text = rm_useless_spaces(text)\n",
        "    text = text.strip()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MiUlTSBB9Wx6",
      "metadata": {
        "id": "MiUlTSBB9Wx6"
      },
      "source": [
        "### 1.2 Tokenize using NLTK\n",
        "\n",
        "We will use our rule-based `clean_text` function to clean our raw text, then use the popular NLTK [punkt tokenizer](https://www.nltk.org/_modules/nltk/tokenize/punkt.html) to convert text to individual sub-words. This will take a while because you have to download the pre-trained punkt tokenizer.\n",
        "\n",
        "*If you are interested: There's a [long and diverse history of converting raw text to \"tokens\"](https://arxiv.org/abs/2112.10508), and many available methods/algorithms (you can experiment with some recently trained ones, trained on a dynamic programming-based method called BPE, [here](https://huggingface.co/spaces/Xenova/the-tokenizer-playground)).*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "id": "vqtdrhF8FEIZ",
      "metadata": {
        "id": "vqtdrhF8FEIZ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to C:\\Users\\Wei\n",
            "[nltk_data]     Xuan\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to C:\\Users\\Wei\n",
            "[nltk_data]     Xuan\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# ===========================================================================\n",
        "# Tokenize using punkt. Don't modify anything in this cell.\n",
        "# ===========================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from tqdm import tqdm\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')\n",
        "df = pd.read_csv(\"OnionOrNot.csv\")\n",
        "df[\"tokenized\"] = df[\"text\"].apply(lambda x: nltk.word_tokenize(clean_text(x.lower())))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qBBdVOYxFEIa",
      "metadata": {
        "id": "qBBdVOYxFEIa"
      },
      "source": [
        "We will use `pandas`, a popular library for data analysis and table manipulation, in this project to manage the dataset. For more information on usage, please refer to the [Pandas documentation](https://pandas.pydata.org/docs/).\n",
        "\n",
        "The primary data structure in Pandas is a `DataFrame`. The following cell will print out the basic information contained in our `DataFrame` structure, and the first few rows of our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "id": "sJjScqV3FEIb",
      "metadata": {
        "id": "sJjScqV3FEIb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>tokenized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Entire Facebook Staff Laughs As Man Tightens P...</td>\n",
              "      <td>1</td>\n",
              "      <td>[entire, facebook, staff, laughs, as, man, tig...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Muslim Woman Denied Soda Can for Fear She Coul...</td>\n",
              "      <td>0</td>\n",
              "      <td>[muslim, woman, denied, soda, can, for, fear, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Bold Move: Hulu Has Announced That They’re Gon...</td>\n",
              "      <td>1</td>\n",
              "      <td>[bold, move, :, hulu, has, announced, that, th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Despondent Jeff Bezos Realizes He’ll Have To W...</td>\n",
              "      <td>1</td>\n",
              "      <td>[despondent, jeff, bezos, realizes, he, ’, ll,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>For men looking for great single women, online...</td>\n",
              "      <td>1</td>\n",
              "      <td>[for, men, looking, for, great, single, women,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  label  \\\n",
              "0  Entire Facebook Staff Laughs As Man Tightens P...      1   \n",
              "1  Muslim Woman Denied Soda Can for Fear She Coul...      0   \n",
              "2  Bold Move: Hulu Has Announced That They’re Gon...      1   \n",
              "3  Despondent Jeff Bezos Realizes He’ll Have To W...      1   \n",
              "4  For men looking for great single women, online...      1   \n",
              "\n",
              "                                           tokenized  \n",
              "0  [entire, facebook, staff, laughs, as, man, tig...  \n",
              "1  [muslim, woman, denied, soda, can, for, fear, ...  \n",
              "2  [bold, move, :, hulu, has, announced, that, th...  \n",
              "3  [despondent, jeff, bezos, realizes, he, ’, ll,...  \n",
              "4  [for, men, looking, for, great, single, women,...  "
            ]
          },
          "execution_count": 152,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# View the first few entries of our dataset\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "D9b4W9z1XhgS",
      "metadata": {
        "id": "D9b4W9z1XhgS"
      },
      "source": [
        "Try to guess some examples! Is the task more difficult than you expected?\n",
        "\n",
        "DataFrames can be indexed using [`.iloc[]`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html). `iloc` uses interger based indexing and supports a single integer (`df.iloc[42]`), a list of integers (`df.iloc[[1, 5, 42]]`), or a slice (`df.iloc[7:42]`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "id": "Ntm8laX6FEIb",
      "metadata": {
        "id": "Ntm8laX6FEIb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "text         Customers continued to wait at drive-thru even...\n",
              "label                                                        0\n",
              "tokenized    [customers, continued, to, wait, at, drive-thr...\n",
              "Name: 42, dtype: object"
            ]
          },
          "execution_count": 153,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# E.g., get row 42 of our dataset\n",
        "df.iloc[42]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TQVT6HUA9htQ",
      "metadata": {
        "id": "TQVT6HUA9htQ"
      },
      "source": [
        "### 1.3 Split the dataset into training, validation, and testing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GDI72x8XFEIc",
      "metadata": {
        "id": "GDI72x8XFEIc"
      },
      "source": [
        "**Train/Test/Val Split** - Now that we've loaded this dataset, we need to split the data into train, validation, and test sets.\n",
        "\n",
        "A good explanation of why we need these different sets can be found in $\\S$2.2.5 of [Eisenstein](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf) but our high-level goal is to have a generalized model and have confidence in our results.\n",
        "\n",
        "\n",
        "The *training set* is used to fit our model's learned parameters (weights and biases) to the task. The *validation  set* (sometimes called development set) is used to verify our training jobs are minimizing loss on an unseen subset of the data and can also be used to help choose hyperparameters for our training setup. The *test set* is used to provide a final evaluation of our trained model (unbiased by development or training decisions), ideally providing some insight into how the model will perform in a scenario we cannot perfectly represent in our data (i.e., the real world). *Each of these sets should be disjoint from the others*, to prevent any leackage that could introduce bias in our evaluation metrics (in this case accuracy).\n",
        "\n",
        "**Model Vocabulary** - We cannot directly feed sub-word token strings into a model! We need to create a \"vocab map\", which contains an ID for each unique token in our Onion dataset. This will be used as a \"lookup\" in the next few sections, since your PyTorch implementation will require first converting your Onion token representations to a list of sub-word IDs.\n",
        "\n",
        "**In the following cell, please implement `split_train_val_test` and `generate_vocab_map`.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "id": "zeo9kX6i9pbH",
      "metadata": {
        "id": "zeo9kX6i9pbH"
      },
      "outputs": [],
      "source": [
        "# ===========================================================================\n",
        "# Set constants for PAD and UNK. You will use these values, but DO NOT change\n",
        "# them, or import additional packages.\n",
        "\n",
        "from collections import Counter\n",
        "PADDING_VALUE = 0\n",
        "UNK_VALUE     = 1\n",
        "\n",
        "# ===========================================================================\n",
        "\n",
        "\n",
        "def split_train_val_test(df, props=[.8, .1, .1]):\n",
        "    \"\"\"\n",
        "    This method takes a dataframe and splits it into train/val/test splits.\n",
        "    It uses the props argument to split the dataset appropriately.\n",
        "\n",
        "    Args:\n",
        "      df (pd.DataFrame): A dataset as a Pandas DataFrame\n",
        "      props (list): Proportions for each split in the order of [train, validation, test].\n",
        "                    the last value of the props array is repetitive, but we've kept it for clarity.\n",
        "\n",
        "    Returns:\n",
        "      train_df (pd.DataFrame): Train DataFrame split.\n",
        "      val_df (pd.DataFrame): Validation DataFrame split.\n",
        "      test_df (pd.DataFrame): Test DataFramem split.\n",
        "    \"\"\"\n",
        "    assert round(sum(props), 2) == 1 and len(props) >= 2\n",
        "    train_df, test_df, val_df = None, None, None\n",
        "\n",
        "    ### BEGIN YOUR CODE (~3-5 lines) ###\n",
        "    ### Hint: You can use df.iloc to slice into specific indexes or ranges.\n",
        "    length = len(df)\n",
        "    train_df = df.iloc[:int(props[0]*length)]\n",
        "    val_df = df.iloc[int(props[0]*length):int(props[0]*length + props[1]*length)]\n",
        "    test_df = df.iloc[int(props[0]*length + props[1]*length):]\n",
        "    ### END YOUR CODE ###\n",
        "\n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "\n",
        "def generate_vocab_map(df, cutoff=2):\n",
        "    \"\"\"\n",
        "    This method takes a dataframe and builds a vocabulary to unique number map.\n",
        "    It uses the cutoff argument to remove rare words occuring <= cutoff times.\n",
        "    *NOTE*: \"\" and \"UNK\" are reserved tokens in our vocab that will be useful\n",
        "    later. You'll also find the Counter imported for you to be useful as well.\n",
        "\n",
        "    Args:\n",
        "      df (pd.DataFrame): The entire dataset this mapping is built from\n",
        "      cutoff (int): We exclude words from the vocab that appear less than or\n",
        "                    eq to cutoff\n",
        "\n",
        "    Returns:\n",
        "      vocab (dict[str, int]):\n",
        "        In vocab, each str is a unique token, and each dict[str] is a\n",
        "        unique integer ID. Only elements that appear > cutoff times appear\n",
        "        in vocab.\n",
        "\n",
        "      reversed_vocab (dict[int, str]):\n",
        "        A reversed version of vocab, which allows us to retrieve\n",
        "        words given their unique integer ID. This map will\n",
        "        allow us to \"decode\" integer sequences we'll encode using\n",
        "        vocab!\n",
        "    \"\"\"\n",
        "\n",
        "    vocab          = {\"\": PADDING_VALUE, \"UNK\": UNK_VALUE}\n",
        "    reversed_vocab = None\n",
        "\n",
        "    ### BEGIN YOUR CODE (~5-15 lines) ###\n",
        "    ### Hint: Start by iterating over df[\"tokenized\"]\n",
        "    c = Counter()\n",
        "\n",
        "    for i in df[\"tokenized\"]:\n",
        "        c.update(i)\n",
        "\n",
        "    for i in c:\n",
        "        if c[i] > cutoff:\n",
        "            vocab[i] = len(vocab)\n",
        "\n",
        "    reversed_vocab = {v:k for k, v in vocab.items()}   \n",
        "\n",
        "    ### END YOUR CODE ###\n",
        "\n",
        "    return vocab, reversed_vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "w9LEk83hRFgT",
      "metadata": {
        "id": "w9LEk83hRFgT"
      },
      "source": [
        "With the methods you have implemented above, we can now split the dataset into training, validation, and testing sets and generate our dictionaries mapping from word tokens to IDs (and vice versa).\n",
        "\n",
        "*Note: The props list currently being used splits the dataset so that 80% of samples are used to train, and the remaining 20% are evenly split between training and validation. How you split your dataset is itself a major choice and something you would need to consider in your own projects. Can you think of why?*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "id": "rcmX931OFEId",
      "metadata": {
        "id": "rcmX931OFEId"
      },
      "outputs": [],
      "source": [
        "df                         = df.sample(frac=1)\n",
        "train_df, val_df, test_df  = split_train_val_test(df, props=[.8, .1, .1])\n",
        "train_vocab, reverse_vocab = generate_vocab_map(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "id": "CAACzA8YFEId",
      "metadata": {
        "id": "CAACzA8YFEId"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.8, 0.1, 0.1)"
            ]
          },
          "execution_count": 156,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ===========================================================================\n",
        "# This line of code will help test your implementation, the expected output is\n",
        "# the same distribution used in 'props' in the above cell. Try out some\n",
        "# different values to ensure it works, but for submission ensure you use\n",
        "# [.8, .1, .1]\n",
        "# ===========================================================================\n",
        "\n",
        "(len(train_df) / len(df)), (len(val_df) / len(df)), (len(test_df) / len(df))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fCFfEHv1hnI",
      "metadata": {
        "id": "5fCFfEHv1hnI"
      },
      "source": [
        "### 1.4 Building a Dataset Class"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8-qTQQa2FEIe",
      "metadata": {
        "id": "8-qTQQa2FEIe"
      },
      "source": [
        "PyTorch has custom Dataset Classes that have very useful extentions, we want to turn our current pandas DataFrame into a subclass of Dataset so that we can iterate and sample through it for minibatch updates. **In the following cell, fill out the `HeadlineDataset` class.** Refer to PyTorch documentation on [Dataset Classes](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n",
        "for help."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "id": "tqt9q92J1QKK",
      "metadata": {
        "id": "tqt9q92J1QKK"
      },
      "outputs": [],
      "source": [
        "# ===========================================================================\n",
        "# Please do not change, or import additional packages.\n",
        "from torch.utils.data import Dataset\n",
        "# ===========================================================================\n",
        "\n",
        "class HeadlineDataset(Dataset):\n",
        "  \"\"\"\n",
        "  This class takes a Pandas DataFrame and wraps in a PyTorch Dataset.\n",
        "  Read more about Torch Datasets here:\n",
        "  https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, vocab, df, max_length=50):\n",
        "    \"\"\"\n",
        "    Initialize this class with appropriate instance variables\n",
        "\n",
        "    We would *strongly* recommend storing the dataframe itself as an instance\n",
        "    variable, and keeping this method very simple. Leave processing to\n",
        "    __getitem__.\n",
        "\n",
        "    Sometimes, however, it does make sense to preprocess in __init__. If you\n",
        "    are curious as to why, read the aside at the bottom of this cell.\n",
        "    \"\"\"\n",
        "\n",
        "    ### BEGIN YOUR CODE (~3 lines) ###\n",
        "    self.vocab = vocab\n",
        "    self.df = df\n",
        "    self.maxLength = max_length\n",
        "    return\n",
        "    ### END YOUR CODE ###\n",
        "\n",
        "  def __len__(self):\n",
        "    \"\"\"\n",
        "    Return the length of the dataframe instance variable\n",
        "    \"\"\"\n",
        "    df_len = None\n",
        "\n",
        "    ### BEGIN YOUR CODE (~1 line) ###\n",
        "    df_len = len(self.df)\n",
        "    ### END YOUR CODE ###\n",
        "\n",
        "    return df_len\n",
        "\n",
        "  def __getitem__(self, index: int):\n",
        "    \"\"\"\n",
        "    Converts a dataframe row (row[\"tokenized\"]) to an encoded torch LongTensor,\n",
        "    using our vocab map created using generate_vocab_map. Restricts the encoded\n",
        "    headline length to max_length.\n",
        "\n",
        "    The purpose of this method is to convert the row - a list of words - into\n",
        "    a corresponding list of numbers.\n",
        "\n",
        "    i.e. using a map of {\"hi\": 2, \"hello\": 3, \"UNK\": 0}\n",
        "    this list [\"hi\", \"hello\", \"NOT_IN_DICT\"] will turn into [2, 3, 0]\n",
        "\n",
        "    Returns:\n",
        "      tokenized_word_tensor (torch.LongTensor):\n",
        "        A 1D tensor of type Long, that has each token in the dataframe mapped to\n",
        "        a number. These numbers are retrieved from the vocab_map we created in\n",
        "        generate_vocab_map.\n",
        "\n",
        "        **IMPORTANT**: if we filtered out the word because it's infrequent (and\n",
        "        it doesn't exist in the vocab) we need to replace it w/ the UNK token.\n",
        "\n",
        "      curr_label (int):\n",
        "        Binary 0/1 label retrieved from the DataFrame.\n",
        "\n",
        "    \"\"\"\n",
        "    tokenized_word_tensor = None\n",
        "    curr_label            = None\n",
        "\n",
        "    ### BEGIN YOUR CODE (~3-7 lines) ###\n",
        "    temp = []\n",
        "    for i in self.df.iloc[index]['tokenized']:\n",
        "        temp.append(self.vocab.get(i, UNK_VALUE))\n",
        "    if len(temp) > self.maxLength:\n",
        "        temp = temp[:self.maxLength]\n",
        "    else:\n",
        "        temp = temp + [PADDING_VALUE] * (self.maxLength - len(temp))\n",
        "    tokenized_word_tensor = torch.LongTensor(temp)\n",
        "    curr_label = self.df.iloc[index]['label']\n",
        "    ### END YOUR CODE ###\n",
        "\n",
        "    return tokenized_word_tensor, curr_label\n",
        "\n",
        "\n",
        "# ===========================================================================\n",
        "# Completely optional aside on preprocessing in __init__.\n",
        "#\n",
        "# Sometimes the compute bottleneck actually ends up being in __getitem__.\n",
        "# In this case, you'd loop over your dataset in __init__, passing data\n",
        "# to __getitem__ and storing it in another instance variable. Then,\n",
        "# you can simply return the preprocessed data in __getitem__ instead of\n",
        "# doing the preprocessing.\n",
        "#\n",
        "# There is a tradeoff though: can you think of one?\n",
        "# ==========================================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "id": "KuLtIOAZFEIe",
      "metadata": {
        "id": "KuLtIOAZFEIe"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import RandomSampler\n",
        "\n",
        "train_dataset = HeadlineDataset(train_vocab, train_df)\n",
        "val_dataset   = HeadlineDataset(train_vocab, val_df)\n",
        "test_dataset  = HeadlineDataset(train_vocab, test_df)\n",
        "\n",
        "# Now that we're wrapping our dataframes in PyTorch datsets, we can make use of\n",
        "# PyTorch Random Samplers, they'll define how our DataLoaders sample elements\n",
        "# from the HeadlineDatasets\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "val_sampler   = RandomSampler(val_dataset)\n",
        "test_sampler  = RandomSampler(test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "n9iBiSKF1yXA",
      "metadata": {
        "id": "n9iBiSKF1yXA"
      },
      "source": [
        "### 1.5 Finalizing our DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lfXSbxoFFEIe",
      "metadata": {
        "id": "lfXSbxoFFEIe"
      },
      "source": [
        "We can now use PyTorch `DataLoader` to batch our data for us. **In the following cell, please implement `collate_fn`.** Refer to PyTorch documentation on [`DataLoader`](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) for help."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "id": "Zp1aQAvn1_mz",
      "metadata": {
        "id": "Zp1aQAvn1_mz"
      },
      "outputs": [],
      "source": [
        "# ===========================================================================\n",
        "# Please do not change, or import additional packages.\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "# ===========================================================================\n",
        "\n",
        "def collate_fn(batch, padding_value=PADDING_VALUE):\n",
        "  \"\"\"\n",
        "  This function is passed as a parameter to Torch DataSampler. collate_fn collects\n",
        "  batched rows, in the form of tuples, from a DataLoader and applies some final\n",
        "  pre-processing.\n",
        "\n",
        "  Objective:\n",
        "    In our case, we need to take the batched input array of 1D tokenized_word_tensors,\n",
        "    and create a 2D tensor that's padded to be the max length from all our tokenized_word_tensors\n",
        "    in a batch. We're moving from a Python array of tuples, to a padded 2D tensor.\n",
        "\n",
        "    *HINT*: you're allowed to use torch.nn.utils.rnn.pad_sequence (ALREADY IMPORTED)\n",
        "\n",
        "    Finally, you can read more about collate_fn here: https://pytorch.org/docs/stable/data.html\n",
        "\n",
        "  Args:\n",
        "    batch: PythonArray[tuple(tokenized_word_tensor: 1D Torch.LongTensor, curr_label: int)]\n",
        "           len(batch) == BATCH_SIZE\n",
        "\n",
        "  Returns:\n",
        "    padded_tokens: 2D LongTensor of shape (BATCH_SIZE, max len of all tokenized_word_tensor))\n",
        "    y_labels: 1D FloatTensor of shape (BATCH_SIZE)\n",
        "\n",
        "  \"\"\"\n",
        "  padded_tokens, y_labels = None, None\n",
        "\n",
        "  ### BEGIN YOUR CODE (~4-8 lines) ###\n",
        "  token_tensor = [i[0] for i in batch]\n",
        "  labels = [i[1] for i in batch]\n",
        "  padded_tokens = pad_sequence(token_tensor, batch_first = True, padding_value=padding_value)\n",
        "  y_labels = torch.FloatTensor(labels)\n",
        "  ### END YOUR CODE ###\n",
        "\n",
        "  return padded_tokens, y_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "id": "OayoJRTeFEIf",
      "metadata": {
        "id": "OayoJRTeFEIf"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, collate_fn=collate_fn)\n",
        "val_iterator   = DataLoader(val_dataset, batch_size=BATCH_SIZE, sampler=val_sampler, collate_fn=collate_fn)\n",
        "test_iterator  = DataLoader(test_dataset, batch_size=BATCH_SIZE, sampler=test_sampler, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "id": "pidbg12AFEIf",
      "metadata": {
        "id": "pidbg12AFEIf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[6222, 3096,    1,   46, 2201,  402,  303,  330,  937, 6148,  223,   97,\n",
            "         2297,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0],\n",
            "        [4880,  112, 1143, 4318, 2275,    8, 6069,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0],\n",
            "        [  86,  318, 1338, 3857, 7158,  160, 1362,   16,    1,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0],\n",
            "        [2471,  259,  885,  886,  221,   97,    1, 1496,   16, 1777,  661,  230,\n",
            "         6781,   14, 5977,  885,  886,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0],\n",
            "        [ 385, 4112, 3736, 2589, 1773, 3655, 1788,  160,    1, 3856,   66,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0],\n",
            "        [ 158,   91, 5292,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0],\n",
            "        [5331,  560, 1034, 2473,   18, 8156,  106, 2802, 7333,    1,    1,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0],\n",
            "        [ 186, 2960, 7488,    4,  891,   14,    1,  937,  410,  207,   54,    1,\n",
            "         5662,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0],\n",
            "        [ 459,  460,  152,  884,  416,  789,  115,    1,   66,  115,  542,    1,\n",
            "           66,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0],\n",
            "        [3027,   20,    1, 3294,  293,   43, 3711,   14,   60, 3060,  106, 1771,\n",
            "           14, 6254, 5131,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0],\n",
            "        [   1,  119, 2168,    4, 2335,   16, 1395,    1, 1050, 1672,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0],\n",
            "        [2186, 8923, 4422, 4423,  599,  561,   97, 1921,   20,  270,    1,   86,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0],\n",
            "        [  54,   63,   29,  395,   14,  215,  581,  753, 4955,  115,  221, 2075,\n",
            "           14,    1, 6235,  448, 2725, 8471,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0],\n",
            "        [1484, 1485,  152,  366, 3580,   91,  785, 1456,   20,  104, 2227,  399,\n",
            "           78,  339,  366,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0],\n",
            "        [ 849,  671,  261,  764, 1924, 3078,  456,  237, 7019, 1042,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0],\n",
            "        [ 785,  152, 3210,  419,   12,   40,   52, 1938, 5092,   18, 1119, 1771,\n",
            "          397, 8399,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0]]) tensor([0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.])\n",
            "x: torch.Size([16, 50])\n",
            "y: torch.Size([16])\n"
          ]
        }
      ],
      "source": [
        "# ===========================================================================\n",
        "# Use this to test your collate_fn implementation.\n",
        "#\n",
        "# You can look at the shapes of x and y or put print statements in collate_fn\n",
        "# while running this snippet\n",
        "# ===========================================================================\n",
        "\n",
        "for x, y in test_iterator:\n",
        "    print(x, y)\n",
        "    print(f'x: {x.shape}')\n",
        "    print(f'y: {y.shape}')\n",
        "    break\n",
        "test_iterator = DataLoader(test_dataset, batch_size=BATCH_SIZE, sampler=test_sampler, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BWLK7T1uFEIg",
      "metadata": {
        "id": "BWLK7T1uFEIg"
      },
      "source": [
        "## 2. Modeling [10 points]\n",
        "Now that we have a clean dataset and a useful PyTorch `DataLoader` object, we can begin building a model for our task! In the following code block, you will build a feed-forward neural network implementing a neural bag-of-words baseline, `NBOW-RAND`, described in $\\S$2.1 of [this paper](https://www.aclweb.org/anthology/P15-1162.pdf). You may find [the PyTorch `torch.nn` docs](https://pytorch.org/docs/stable/nn.html) useful for understanding the different layers and [this PyTorch sequence models tutorial](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html) for how to put together `torch.nn` layers.\n",
        "\n",
        "The core intuition behind `NBOW-RAND` is that after we embed each word for our input, we average the embeddings to produce a single vector that hopefully averages the information across all embeddings. Formally, we first convert each document of length $n$ tokens into a matrix of $n\\times d$, where $d$ is the dimension of the token embedding. Then we average all embeddings to produce a vector of length $d$.\n",
        "\n",
        "If you are new to PyTorch, ensuring your matrix operations are correct is often the most common source of errors. Keep in mind how the dimensions change and what each axes represents. Your documents will be passed in as minibatches, so be careful when selecting which axes to apply certain operations. Feel free to experiment with the architecture of this network outside of the basic `NBOW-RAND` setup (such as adding in other layers) to see how this changes your results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pZDPs0Sf-H3V",
      "metadata": {
        "id": "pZDPs0Sf-H3V"
      },
      "source": [
        "### 2.1 Define the NBOW model class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "id": "jzGx2q0jLqyU",
      "metadata": {
        "id": "jzGx2q0jLqyU"
      },
      "outputs": [],
      "source": [
        "# ===========================================================================\n",
        "# Please do not change, or import additional packages.\n",
        "import torch.nn as nn\n",
        "# ===========================================================================\n",
        "\n",
        "class NBOW(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim):\n",
        "    \"\"\"\n",
        "    Instantiate layers for your model.\n",
        "    Your model architecture will be a feed-forward neural network.\n",
        "\n",
        "    You will need 3 nn.Modules at minimum\n",
        "     1. An embeddings layer (see nn.Embedding)\n",
        "     2. A linear layer (see nn.Linear)\n",
        "     3. A sigmoid output (see nn.Sigmoid)\n",
        "\n",
        "    HINT: In the forward step, the BATCH_SIZE is the first dimension.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    ### BEGIN YOUR CODE (~4 lines) ###\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.linear = nn.Linear(embedding_dim, 1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    ### END YOUR CODE ###\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Complete the forward pass of the model.\n",
        "\n",
        "    Use the output of the embedding layer to create the average vector,\n",
        "    which will be input into the linear layer.\n",
        "\n",
        "    Args:\n",
        "      x: 2D LongTensor of shape (BATCH_SIZE, max len of all tokenized_word_tensor))\n",
        "         This is the same output that comes out of the collate_fn function you completed\n",
        "    \"\"\"\n",
        "    ### BEGIN YOUR CODE (~4-5 lines) ###\n",
        "    x = self.embedding(x)\n",
        "    x = torch.mean(x, dim = 1)\n",
        "    x = self.linear(x)\n",
        "    x = self.sigmoid(x)\n",
        "\n",
        "    return x\n",
        "    ### END YOUR CODE ###\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xltosIzM-SP2",
      "metadata": {
        "id": "xltosIzM-SP2"
      },
      "source": [
        "### 2.2 Initialize the NBOW classification model\n",
        "\n",
        "Since the NBOW model is rather basic, there is only one meaningful hyperparameter w.r.t. model architecture: the size of the embedding dimension (`embedding_dim`). (We also see a `vocab_size` parameter here, but this only a by-product on our cutoff for infrequent tokens, there also may more hyperparameters if you modified the architecture, such as adding a linear layer).\n",
        "\n",
        "Remember the CUDA discussion in the first cell of this notebook? Here the `.to(device)` is where that discussion becomes relevant (if `device=='cuda'`, PyTorch will perform the matrix operations on GPU). If you recieve a mismatch error, your tensors may be on different devices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "id": "_HQWUu-ZFEIg",
      "metadata": {
        "id": "_HQWUu-ZFEIg"
      },
      "outputs": [],
      "source": [
        "model = NBOW(\n",
        "  vocab_size    = len(train_vocab.keys()),\n",
        "  embedding_dim = 300\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "C4CZnj1f-da-",
      "metadata": {
        "id": "C4CZnj1f-da-"
      },
      "source": [
        "### 2.3 Instantiate the loss function and optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9aXi8nA0FEIh",
      "metadata": {
        "id": "9aXi8nA0FEIh"
      },
      "source": [
        "Please select and instantiate an appropriate loss function and optimizer.\n",
        "\n",
        "*Hint: What loss functions are availible for binary classification? Feel free to look at the [torch.nn docs on loss functions](https://pytorch.org/docs/stable/nn.html#loss-functions) for help!*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "id": "w98UvlXxFEIh",
      "metadata": {
        "id": "w98UvlXxFEIh"
      },
      "outputs": [],
      "source": [
        "# While we import Adam for you, you may try / import other optimizers as well\n",
        "from torch.optim import Adam\n",
        "\n",
        "criterion, optimizer = None, None\n",
        "\n",
        "### BEGIN YOUR CODE ###\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = Adam(model.parameters())\n",
        "### END YOUR CODE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hUXBtqPEjiRe",
      "metadata": {
        "id": "hUXBtqPEjiRe"
      },
      "source": [
        "Now that we have a NBOW model, a loss function, optimizer and dataset, we can begin training!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bVLeTa8wFEIh",
      "metadata": {
        "id": "bVLeTa8wFEIh"
      },
      "source": [
        "## 3. Training and Evaluation [10 points]\n",
        "We will now instantiate a `train_loop`, and a `val_loop` to evaluate our model at each epoch.\n",
        "\n",
        "**Fill out the train and test loops below. Treat real headlines as `False`, and Onion headlines as `True`.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "id": "vganx5fCFEIh",
      "metadata": {
        "id": "vganx5fCFEIh"
      },
      "outputs": [],
      "source": [
        "def train_loop(model, criterion, optim, iterator):\n",
        "  \"\"\"\n",
        "  Returns the total loss calculated from criterion\n",
        "  \"\"\"\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  for x, y in tqdm(iterator):\n",
        "    ### BEGIN YOUR CODE (~6 lines) ###\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    optim.zero_grad()\n",
        "    y_pred = model(x)\n",
        "    loss = criterion(y_pred, y.unsqueeze(1))\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "    ### END YOUR CODE ###\n",
        "\n",
        "  return total_loss\n",
        "\n",
        "\n",
        "def val_loop(model, iterator):\n",
        "  \"\"\"\n",
        "  Returns:\n",
        "    true (List[bool]): All the ground truth values taken from the dataset iterator\n",
        "    pred (List[bool]): All model predictions.\n",
        "  \"\"\"\n",
        "  true, pred = [], []\n",
        "\n",
        "  ### BEGIN YOUR CODE (~8 lines) ###\n",
        "  model.eval()\n",
        "\n",
        "  for x, y in tqdm(iterator):\n",
        "    y_pred = model(x)\n",
        "    pred.extend(y_pred.round().tolist())\n",
        "    true.extend(y.bool().tolist())\n",
        "\n",
        "  ### END YOUR CODE ###\n",
        "\n",
        "  return true, pred"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JNXJevTu-tDZ",
      "metadata": {
        "id": "JNXJevTu-tDZ"
      },
      "source": [
        "### 3.1 Define the evaluation metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7IsZQs3rFEIi",
      "metadata": {
        "id": "7IsZQs3rFEIi"
      },
      "source": [
        "We will also need evaluation metrics to tell us how well our model is doing on the validation set at each epoch and later how well the model does on the held-out test set. You may find $\\S$4.4.1 of Eisenstein useful for these questions.\n",
        "\n",
        "**Complete the functions in the following cell.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "id": "gMQDg9Vy-wY0",
      "metadata": {
        "id": "gMQDg9Vy-wY0"
      },
      "outputs": [],
      "source": [
        "# Note: You will not need to import anything to implement these functions.\n",
        "\n",
        "def accuracy(true, pred):\n",
        "  \"\"\"\n",
        "  Calculate the ratio of correct predictions.\n",
        "\n",
        "  Args:\n",
        "    true (List[bool]): ground truth\n",
        "    pred (List[bool]): model predictions\n",
        "\n",
        "  Returns:\n",
        "    acc (float): percent accuracy with range [0, 1]\n",
        "  \"\"\"\n",
        "  acc = None\n",
        "  ### BEGIN YOUR CODE (~2-5 lines) ###\n",
        "  pred = [i[0] for i in pred]\n",
        "  num_correct = 0\n",
        "\n",
        "  for i in range(len(true)):\n",
        "      if true[i] == pred[i]:\n",
        "         num_correct += 1\n",
        "  acc = num_correct / len(true)\n",
        "\n",
        "\n",
        "  ### END YOUR CODE ###\n",
        "  return acc\n",
        "\n",
        "\n",
        "def binary_f1(true, pred, selected_class=True):\n",
        "  \"\"\"\n",
        "  Calculate F-1 scores for a binary classification task.\n",
        "\n",
        "  Args:\n",
        "    true (List[bool]): ground truth\n",
        "    pred (List[bool]): model predictions\n",
        "    selected_class (bool): the selected class the F-1 is being calculated for.\n",
        "\n",
        "  Returns:\n",
        "    f1 (float): F-1 score between [0, 1]\n",
        "  \"\"\"\n",
        "  f1 = None\n",
        "  ### BEGIN YOUR CODE (~10-15 lines) ###\n",
        "  tp, fp, fn = 0, 0, 0\n",
        "  pred = [i[0] for i in pred]\n",
        "\n",
        "  for i in range(len(true)):\n",
        "      if pred[i] == selected_class:\n",
        "        if true[i] == selected_class:\n",
        "           tp += 1\n",
        "        else:\n",
        "           fp += 1\n",
        "      elif true[i] == selected_class:\n",
        "        fn += 1\n",
        "  \n",
        "  precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "  recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "\n",
        "  f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "  ### END YOUR CODE ###\n",
        "  return f1\n",
        "\n",
        "\n",
        "def binary_macro_f1(true, pred):\n",
        "  \"\"\"\n",
        "  Calculate averaged F-1 for all selected (true/false) classes.\n",
        "\n",
        "  Args:\n",
        "    true (List[bool]): ground truth\n",
        "    pred (List[bool]): model predictions\n",
        "  \"\"\"\n",
        "  averaged_macro_f1 = None\n",
        "  ### BEGIN YOUR CODE (~1 line) ###\n",
        "  averaged_macro_f1 = (binary_f1(true, pred, True) + binary_f1(true, pred, False)) / 2\n",
        "\n",
        "  ### END YOUR CODE ###\n",
        "  return averaged_macro_f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "id": "Yw79JFieFEIi",
      "metadata": {
        "id": "Yw79JFieFEIi"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 150/150 [00:00<00:00, 338.52it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Binary Macro F1: 0.2707383773928897\n",
            "Accuracy: 0.37125\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ===========================================================================\n",
        "# To test your eval implementation, we will evaluate the untrained model on our\n",
        "# dev dataset. It will do pretty poorly (it's untrained), but the exact performance\n",
        "# will be random since the initialization of the model parameters is random.\n",
        "# ===========================================================================\n",
        "\n",
        "true, pred = val_loop(model, val_iterator)\n",
        "print(f'Binary Macro F1: {binary_macro_f1(true, pred)}')\n",
        "print(f'Accuracy: {accuracy(true, pred)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Q2to0kWVFEIi",
      "metadata": {
        "id": "Q2to0kWVFEIi"
      },
      "source": [
        "## 4. Full Training Run [1 point]\n",
        "Now we can perform a full run and see the model fit our loss. If everything goes correctly, you should be able to achieve a validation F1 score of at least `0.80`\n",
        "\n",
        "**Feel free to adjust the number of epochs to prevent overfitting or underfitting and to play with your model hyperparameters/optimizer & loss function.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "id": "N-iuqkKCFEIj",
      "metadata": {
        "id": "N-iuqkKCFEIj"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1200/1200 [00:14<00:00, 85.67it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 495.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH: 0\n",
            "TRAIN LOSS: 677.1727117598057\n",
            "VAL F-1: 0.7426842290579556\n",
            "VAL ACC: 0.7829166666666667\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1200/1200 [00:13<00:00, 91.52it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 528.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH: 1\n",
            "TRAIN LOSS: 462.75611308962107\n",
            "VAL F-1: 0.8414387765469298\n",
            "VAL ACC: 0.85125\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1200/1200 [00:13<00:00, 90.17it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 304.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH: 2\n",
            "TRAIN LOSS: 361.79292799532413\n",
            "VAL F-1: 0.8552835005213388\n",
            "VAL ACC: 0.8658333333333333\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1200/1200 [00:13<00:00, 86.24it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 397.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH: 3\n",
            "TRAIN LOSS: 307.1231104284525\n",
            "VAL F-1: 0.8608319158806248\n",
            "VAL ACC: 0.87\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1200/1200 [00:16<00:00, 72.10it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 270.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH: 4\n",
            "TRAIN LOSS: 270.68400552496314\n",
            "VAL F-1: 0.8592179722755053\n",
            "VAL ACC: 0.8691666666666666\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1200/1200 [00:17<00:00, 66.75it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 326.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH: 5\n",
            "TRAIN LOSS: 241.15441867522895\n",
            "VAL F-1: 0.858870309094558\n",
            "VAL ACC: 0.86875\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1200/1200 [00:17<00:00, 68.22it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 345.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH: 6\n",
            "TRAIN LOSS: 217.62708221375942\n",
            "VAL F-1: 0.859418092384102\n",
            "VAL ACC: 0.8691666666666666\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1200/1200 [00:17<00:00, 70.19it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 288.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH: 7\n",
            "TRAIN LOSS: 199.39884072169662\n",
            "VAL F-1: 0.8581872921932667\n",
            "VAL ACC: 0.86875\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1200/1200 [00:18<00:00, 65.94it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 366.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH: 8\n",
            "TRAIN LOSS: 182.3571298168972\n",
            "VAL F-1: 0.8587626996151061\n",
            "VAL ACC: 0.8679166666666667\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1200/1200 [00:16<00:00, 72.56it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 359.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH: 9\n",
            "TRAIN LOSS: 167.30877292482182\n",
            "VAL F-1: 0.8572131688400193\n",
            "VAL ACC: 0.8670833333333333\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "TOTAL_EPOCHS = 10\n",
        "for epoch in range(TOTAL_EPOCHS):\n",
        "    train_loss = train_loop(model, criterion, optimizer, train_iterator)\n",
        "    true, pred = val_loop(model, val_iterator)\n",
        "    print(f\"EPOCH: {epoch}\")\n",
        "    print(f\"TRAIN LOSS: {train_loss}\")\n",
        "    print(f\"VAL F-1: {binary_macro_f1(true, pred)}\")\n",
        "    print(f\"VAL ACC: {accuracy(true, pred)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_l91F4ooFEIj",
      "metadata": {
        "id": "_l91F4ooFEIj"
      },
      "source": [
        "We can also look at the models performance on the held-out test set, using the same `val_loop` from earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "id": "vs8Fy_ncFEIo",
      "metadata": {
        "id": "vs8Fy_ncFEIo"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 150/150 [00:00<00:00, 339.43it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TEST F-1: 0.8614266662854778\n",
            "TEST ACC: 0.87125\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "true, pred = val_loop(model, test_iterator)\n",
        "print(f\"TEST F-1: {binary_macro_f1(true, pred)}\")\n",
        "print(f\"TEST ACC: {accuracy(true, pred)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rMPWmorEFEIp",
      "metadata": {
        "id": "rMPWmorEFEIp"
      },
      "source": [
        "## 5. Analysis [5 points]\n",
        "While modeling and accuracy are a great signal that our model is working in our specific task setup, an inspection of what the model is classifying (particularly its errors), can allow us to hypothesize about what is going on, why it works, and how to improve.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fnjKlKt352hQ",
      "metadata": {
        "id": "fnjKlKt352hQ"
      },
      "source": [
        "### 5.1 Impact of Vocab Size\n",
        "**Question:** *What happens to the vocab size as you change the cutoff in the cell below? Can you explain this in the context of [Zipf's Law](https://en.wikipedia.org/wiki/Zipf%27s_law)?*\n",
        "\n",
        "**Answer:** The size of the vocab decreases as cutoff increases. This is because increasing the cutoff results in only including words that appear at least a certain number of times, hence excluding rare words. Zipf's law notes that frequency of any word is inversely proportional to its rank in the frequency table, where vast majority of words will have lower frequencies and drop-off as we move down the list of ranked words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "id": "pI0fM4oMFEIp",
      "metadata": {
        "id": "pI0fM4oMFEIp"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7623"
            ]
          },
          "execution_count": 170,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tmp_vocab, _ = generate_vocab_map(train_df, cutoff = 3)\n",
        "len(tmp_vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0x54B1lFEIp",
      "metadata": {
        "id": "d0x54B1lFEIp"
      },
      "source": [
        "### 5.2 Error Analysis\n",
        "\n",
        "*Can you describe what cases the model is getting wrong in the witheld test-set?*\n",
        "\n",
        "To do this, you will need to create a new `val_train_loop_incorrect` which returns incorrect sequences **and** you will need to decode these sequences back into words. You have already created a map that can convert encoded sequences back to regular English (`reverse_vocab`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "id": "TfohtPF8FEIp",
      "metadata": {
        "id": "TfohtPF8FEIp"
      },
      "outputs": [],
      "source": [
        "def val_train_loop_incorrect(model, iterator):\n",
        "  \"\"\"\n",
        "  Implement this however you like! It should look very similar to val_loop.\n",
        "  Pass the test_iterator through this function to look at errors in the test set.\n",
        "  \"\"\"\n",
        "\n",
        "  model.eval()\n",
        "  \n",
        "  incorrect = []\n",
        "\n",
        "  for x, y in tqdm(iterator):\n",
        "    y_pred = model(x)\n",
        "    y_pred = y_pred.round()\n",
        "    \n",
        "    for i in range(len(y_pred)):\n",
        "      if y_pred[i] != y[i]:\n",
        "          indices = x[i].tolist()\n",
        "          indices = [i for i in indices if i != PADDING_VALUE]\n",
        "          words = \" \".join([reverse_vocab.get(i, \"UNK\") for i in indices])\n",
        "          \n",
        "          incorrect.append(words)\n",
        "\n",
        "  return incorrect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "id": "6-azPje88iU0",
      "metadata": {
        "id": "6-azPje88iU0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 150/150 [00:00<00:00, 313.75it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['buzzfeed hires clickhole editor',\n",
              " 'is your interior designer putting your life at risk ?',\n",
              " 'new haven police officer UNK like dog , tricks suspects into UNK',\n",
              " 'UNK couple saves money by making own porn',\n",
              " 'measles UNK must pay doctor UNK',\n",
              " 'kelly UNK makes racial UNK while slamming trump for racial UNK',\n",
              " \"government agencies have to tear down their websites , even if it 's cheaper to leave them up\",\n",
              " 'man on verge of UNK instead turns to god',\n",
              " 'UNK corpse of jeremy UNK attends UNK board meeting',\n",
              " 'father of brooklyn teen who died on class field trip gets call asking why son has been UNK',\n",
              " 'seasons turn UNK from the one that kills old people to the one that kills homeless people',\n",
              " 'america has found a way to UNK water',\n",
              " 'melania trump would have been UNK for deportation under new immigration rules',\n",
              " 'UNK fans put colin kaepernick up for sale on amazon',\n",
              " 'the oldest person in the world UNK her long life to eating eggs and being single',\n",
              " 'controversial theory suggests aliens may have built ancient egypt ’ s UNK UNK',\n",
              " 'latest attack : isis just changed its name to ‘ google ’',\n",
              " 'national guard UNK hunted homeless with UNK guns',\n",
              " 'kavanaugh packing gun at congressional hearing in case parkland father tries to shake his hand again',\n",
              " 'usa today : UNK conflict severely limits tourism in afghanistan',\n",
              " 'school administration reminds female students bulletproof UNK must cover UNK',\n",
              " \"by the time UNK harper 's UNK contract UNK , UNK will be a lonely , old , useless UNK\",\n",
              " 'nasa mars rover accidentally draws penis on red planet',\n",
              " 'most americans would like to punch donald trump',\n",
              " 'UNK , christ . not another UNK full of urine .',\n",
              " 'world ’ s top UNK from ‘ UNK is now UNK , will be UNK',\n",
              " 'your horoscopes — week of february 28 , 2017',\n",
              " 'UNK might be hidden homosexuals',\n",
              " 'innocent man ends up UNK with UNK cop that framed him',\n",
              " 'breaking : drunk teen going 100 mph down UNK highway is UNK',\n",
              " 'obese man doesn ’ t understand why he can ’ t lose weight despite his healthy , UNK diet',\n",
              " '‘ UNK ’ tops amazon UNK list',\n",
              " 'UNK mix-up puts tony UNK in middle of UNK',\n",
              " '‘ men are not UNK , ’ says woman who has no idea what it like to take two whole UNK to get to your clothing section at UNK',\n",
              " 'scott pruitt defends use of 1st armored division for trip to UNK',\n",
              " 'new UNK school opens for students with interest in receiving UNK education',\n",
              " 'if first lady michelle obama could pick any other job , ‘ i would be beyonce ’',\n",
              " 'donald trump jr. takes son on hunting trip in national zoo',\n",
              " 'paul ryan ’ s first challenge as house speaker : getting the smell of smoke left by boehner out of the speaker ’ s office',\n",
              " 'people help themselves to UNK of thousands of onions found in middle of desert',\n",
              " 'UNK UNK fends off burglar with back UNK',\n",
              " 'police : student had UNK bad plans for school shooting',\n",
              " 'everyone in pride parade straight',\n",
              " 'ancient UNK erection UNK in amber',\n",
              " 'UNK informs george h.w . bush that dying so soon after wife would really boost UNK rating',\n",
              " 'gang of UNK UNK UNK UNK by UNK police following UNK shop assault',\n",
              " 'white house claims iran behind attack on nancy UNK',\n",
              " 'forgotten sour cream leads to milwaukee fast food shooting',\n",
              " 'earth ’ s last male northern white rhino gets personal armed UNK',\n",
              " 'man in gorilla suit shot with UNK UNK when vet confused him for real gorilla',\n",
              " 'news : a UNK of justice : a shocking study has found that as many as 1 in 10 people burned at the stake for witchcraft is falsely accused',\n",
              " 'UNK : stay with your UNK UNK UNK face ‘ discipline ’',\n",
              " 'UNK to britain : massive power UNK caused by millions of people simultaneously making tea',\n",
              " 'UNK confirms UNK dragon from ‘ UNK ’ pregnant',\n",
              " 'comcast now just calling its customers assholes to their faces',\n",
              " 'the secret to my UNK marriage is trust , respect , and threatening to kill myself if she leaves',\n",
              " 'depression , UNK UNK combine forces to produce UNK UNK UNK',\n",
              " 'forest service considering explosives to get rid of frozen cows in colorado mountain cabin .',\n",
              " 'fema UNK emergency UNK pills for residents stranded by hurricane florence',\n",
              " 'painting hanging in UNK store must be founder of the UNK army',\n",
              " 'study finds medical marijuana effective for UNK long-term pain over jerry UNK ’ s death',\n",
              " 'amputee inspires others not to lose limbs',\n",
              " 'baltimore craigslist poster offering ‘ pokémon go UNK ’ services',\n",
              " 'dea never checked if its massive surveillance operations are legal',\n",
              " 'truck full of UNK UNK on u.s. 101 , UNK UNK with UNK sea creatures',\n",
              " 'pope francis offers molested kids 10 % off at vatican city gift shop',\n",
              " 'kentucky players UNK over losing UNK season bonuses',\n",
              " 'team UNK 2 poster mistaken for us propaganda on russian state television',\n",
              " 'man UNK cashier for putting chips and UNK goods in same grocery bag',\n",
              " 'lean in , queen ! her husband told her the only room off limits in his UNK estate was his private study , but she went in anyway and totally owned her horrible fate',\n",
              " 'i had a terrible experience at this restaurant because i am a terrible person',\n",
              " 'u.s. will not seek to prosecute ancient tribe who murdered UNK john allen UNK',\n",
              " 'kanye west doesn ’ t like appearing on keeping up with the kardashians because he has issues with the UNK',\n",
              " 'ted cruz asks central park UNK cab driver how much it costs to UNK horse for an hour',\n",
              " 'man somehow getting worse at sex',\n",
              " 'neither the time nor place : this girl wrote that she and her boyfriend have had their ups and downs in the UNK of the instagram she posted for his birthday',\n",
              " 'UNK ben UNK claims',\n",
              " 'UNK government UNK students from UNK in showers by handing out digital UNK to limit shower time',\n",
              " 'disturbing teen trend : UNK across the country are getting together weekly to worship a dead man named jesus christ',\n",
              " 'warren buffett eats like a UNK as they have UNK death rate',\n",
              " 'u.s. protects already extinct UNK UNK',\n",
              " '‘ i ’ m going to get you UNK ’ : UNK chases man who UNK her in cambridge',\n",
              " 'life : first we gave this girl a barbie . then we gave her a doll with normal UNK . then we gave her a doll with goat UNK',\n",
              " 'popeyes UNK chick-fil-a UNK with new sandwich featuring dan UNK ’ s battered , fried loved ones',\n",
              " \"doctors say average heart attack victim does n't UNK at chest nearly UNK enough\",\n",
              " 'study : most serial UNK did not receive toy every time they went to store as kids',\n",
              " 'elon musk hires onion writers for project . ( not an onion headline )',\n",
              " \"us taxpayers getting cut of UNK of the christ ' UNK\",\n",
              " 'eric UNK : schools ‘ pushing the liberal agenda ’ by teaching UNK',\n",
              " '‘ the UNK ’ turns 20',\n",
              " 'gop website declares mike pence winner of vp debate before it begins',\n",
              " 'such is life : three spaghetti UNK and two spaghetti wins !',\n",
              " '300,000 pounds of rat meat sold as chicken wings across america',\n",
              " 'panicked , UNK pope UNK UNK ban on abortion',\n",
              " \"hundreds gather to stare at UNK construction site hole and say UNK ' like UNK wilson\",\n",
              " 'UNK duncan spends visit to local elementary school looking at ufo books in library',\n",
              " 'UNK at this mess : french park trains UNK to pick up litter',\n",
              " 'indiana couple UNK goal of visiting every cracker barrel',\n",
              " 'UNK chan UNK UNK UNK solar panel efficiency by a massive 22 %',\n",
              " 'i ’ m so glad uncle joe is UNK again',\n",
              " 'fire UNK factory destroyed in massive blaze',\n",
              " 'part of the 1 % : bernie sanders is UNK in the polls after a dna test revealed that he ’ s king',\n",
              " 'image of kissing UNK has been named the single work of art that best UNK british identity',\n",
              " 'mom UNK into school office and slaps wrong child',\n",
              " 'louis UNK . fan disappointed at lack of UNK power games in new material',\n",
              " 'the roomba for UNK is really UNK off astronomers',\n",
              " 'this guy was loved so much both wife , girlfriend place UNK in newspaper',\n",
              " 'jay-z tried to have UNK ’ s name changed',\n",
              " 'town in new jersey to fine UNK for driving through town',\n",
              " 'UNK legendary actor christopher lee set to unleash a metal album next week',\n",
              " 'guantanamo guards beat a prisoner into brain injuries who later UNK out to be an undercover guard who was taking part in a training exercise',\n",
              " 'UNK hammer not actually a fan of UNK',\n",
              " \"i ca n't stand it when jews talk during movies\",\n",
              " 'UNK battle furiously over jennifer UNK',\n",
              " 'sean spicer given own press secretary to answer media ’ s questions about his controversial statements',\n",
              " '8 families find out they have been paying respects to the wrong UNK for 39 years',\n",
              " 'UNK UNK rocks back and forth in UNK while watching arby ’ s clap back at burger king on twitter',\n",
              " 'study : women fake UNK to increase sexual UNK',\n",
              " 'study : 7 of 10 most UNK u.s. hospitals are UNK',\n",
              " 'japanese family puts aging robot in retirement home',\n",
              " 'habitat for humanity investigated for working conditions after UNK UNK collapses on site',\n",
              " \"trump on black supporter : UNK at my african-american over here '\",\n",
              " \"`` on UNK , it probably was n't the best decision '' - man regrets buying 7,000 lance armstrong UNK tips UNK\",\n",
              " \"UNK UNK out to prove he 's worth UNK contract\",\n",
              " 'former lovers meet in coffee shop for one last UNK',\n",
              " 'ice argues migrants in camps are free to die at any time',\n",
              " 'for the sixth time in one week , man shot at gun show',\n",
              " 'police subject man to 8 anal searches after minor traffic violation',\n",
              " 'UNK black unveils the song we ’ ve all been waiting for : ‘ saturday ’',\n",
              " 'depressed businessman takes 16 power naps a day',\n",
              " 'kellyanne conway decides to lay low until rule of law dies down',\n",
              " 'little miss hispanic delaware stripped of title , because she ’ s not latina enough',\n",
              " 'UNK UNK UNK pray owner gets job soon',\n",
              " \"birth horror : baby 's head torn off during birth\",\n",
              " 'russian lawyer admits to repeatedly informing kremlin of trump campaign ’ s UNK',\n",
              " 'UNK UNK UNK UNK after learning bob UNK wrote a president book without him',\n",
              " 'high court to doctors : write UNK UNK',\n",
              " \"nra says mass shootings just the UNK price of protecting people 's freedom to commit mass shootings\",\n",
              " \"trump : i always UNK that i was in the military '\",\n",
              " \"probe on UNK goat carcass will take up to a month to see if it 's a UNK hybrid : official\",\n",
              " 'UNK UNK crab girlfriend wants to move in',\n",
              " 'boyfriend ’ s UNK an UNK sleeping bag',\n",
              " \"man does n't even do good job at sleeping\",\n",
              " 'goldman sachs hired by russia as corporate broker to boost image',\n",
              " 'e3 attendees flee in terror after bethesda presentation UNK causes UNK to UNK on convention floor',\n",
              " 'butt of their jokes',\n",
              " '‘ grab her hand and put it right on your dick : ’ UNK successfully UNK his terrible dating book',\n",
              " 'real-life UNK : an app ’ s high score UNK someone a cow',\n",
              " \"the top search result on the onion 's website\",\n",
              " 'UNK ‘ jeopardy ! ’ UNK says key to success is threatening other contestants with UNK baseball bat during commercials',\n",
              " 'one dead in hair UNK UNK',\n",
              " 'tree counter is UNK by how many trees there are',\n",
              " 'UNK , on reddit , dudes can ’ t stop talking about fucking UNK',\n",
              " 'UNK father of UNK receives shocking news in the delivery room : there are no babies',\n",
              " 'manager of UNK taco bell / kfc secretly considers it mostly a taco bell',\n",
              " 'alex jones gets coffee dumped on him in seattle after chasing a guy down on the street',\n",
              " 'UNK turtle that UNK through its genitals added to endangered list',\n",
              " 'air force removes god from chain of command',\n",
              " 'milwaukee officer who got drunk , let child drive , up for promotion',\n",
              " '10 cat UNK for the blind',\n",
              " 'large UNK tarantula on the loose',\n",
              " \"self-conscious panda swears it UNK UNK UNK to it as UNK '\",\n",
              " 'nfl reportedly asking music acts to pay for playing super bowl halftime show',\n",
              " 'denver ’ s flaming skull mayor announces plans to UNK magic mushrooms',\n",
              " 'why UNK join isis',\n",
              " 'patriothole : wasting taxpayer money ? the white house reportedly spends $ UNK a month on a free UNK trial obama forgot to cancel',\n",
              " 'should the government stop dumping money into a giant hole ? ( youtube )',\n",
              " '‘ we will not repeat the mistakes of the 2016 election , ’ vows nation still using internet',\n",
              " 'inspiring rescue : this good samaritan in hawaii UNK through UNK to rescue a dog from drowning',\n",
              " 'kidnapped teen freed , though freedom is its own kind of prison , is it not ?',\n",
              " 'health scare prompts man to start UNK healthier',\n",
              " 'trump UNK golf trophy to hurricane victims',\n",
              " 'mark zuckerberg can ’ t believe india isn ’ t grateful for facebook ’ s free internet',\n",
              " '‘ ginger extremist ’ convicted in royal death plot so prince harry can be king',\n",
              " 'millionaire is worried it will be awkward when she demands money from homeless man',\n",
              " 'man practicing open carry law robbed of gun',\n",
              " \"lyrics to carly UNK UNK 's next single to be UNK via online poll\",\n",
              " 'black and latina women scientists sometimes mistaken for UNK',\n",
              " 'perfect UNK does not assault drunk woman',\n",
              " 'report states dr. phil left recovering guest vodka in his dressing room',\n",
              " 'holocaust museum : please stop playing pokémon go here',\n",
              " 'department of education hires art teacher to spread UNK across all u.s. public schools',\n",
              " 'london has already UNK its pollution limits for 2016',\n",
              " 'rock fans outraged as bob UNK goes UNK',\n",
              " 'UNK spiders cause UNK car recall for second time',\n",
              " 'naked man waving american flag leads to large identity theft bust',\n",
              " 'a beautiful reunion : this high school football player got the surprise of a lifetime when he removed his helmet to reveal that he was his father who had been fighting in afghanistan for the past 2 years',\n",
              " 'news : security breach : edward snowden ’ s robot has been UNK into the white house front door for 3 hours straight',\n",
              " 'the onion said bill UNK wanted to join squad of UNK . then , UNK let him in',\n",
              " 'math journal accepts UNK paper UNK by computer program',\n",
              " 'can a mother actually lift a car if her child is trapped under it ?',\n",
              " 'supreme court hears case of woman ticketed for not holding UNK UNK',\n",
              " 'UNK UNK offers UNK cocaine | the onion',\n",
              " 'UNK important for body : expert',\n",
              " '[ theonion ] this is not a dating site . largest in world online search sex partners',\n",
              " 'feeling bad about feeling bad can make you feel worse',\n",
              " 'UNK finally goes too far , removes silent track for copyright infringement',\n",
              " 'african-american neighborhood UNK by ask murderer',\n",
              " 'new express transplant list offers patients kidney or first available UNK',\n",
              " \"' i used to look up to you , ' shouts UNK flynn jr. running out of room after learning father a UNK\",\n",
              " 'man discovers end of UNK UNK after UNK hours of UNK',\n",
              " 'UNK UNK angels target businesses by posting UNK reviews',\n",
              " 'man billed thousands for UNK he never received .',\n",
              " 'lawyers identify dozens more bill cosby victims while UNK potential UNK',\n",
              " '50 % of UNK would rather be UNK by a groundhog in congress',\n",
              " 'new poll finds millennials far more likely to politically identify as UNK than previous generations',\n",
              " 'visitors to chinese zoo feel UNK after discovering new penguin display UNK of UNK toys',\n",
              " 'man pours all his UNK UNK into UNK , removing pizza from oven',\n",
              " 'women still UNK in medical leadership by men with UNK , study finds',\n",
              " 'world of warcraft gamer dies after playing 19 hours straight',\n",
              " '6 things that could ’ ve been bought with the $ 1.5 trillion the government spent developing the UNK fighter jet',\n",
              " \"'the onion ' to halt UNK assault on trees\",\n",
              " 'new york train UNK reports suspicious UNK , turn out to be machines used to report suspicious UNK',\n",
              " 'UNK rich people now have as much UNK as 50 % of the rest of humanity UNK',\n",
              " '‘ to defeat them , i must become them , ’ john kerry says while putting on black face mask',\n",
              " \"victim recalls UNK 's breasts , little else\",\n",
              " \"ten years later , cheney haunted by people he did n't UNK to kill in iraq war\",\n",
              " 'upcoming ‘ game of thrones ’ battle reportedly took 55 days to shoot',\n",
              " 'donald trump stares UNK at tiny , aged penis in mirror before putting on clothes , beginning day',\n",
              " 'study : fat UNK doesn ’ t help obese people lose weight',\n",
              " 'dick cheney vice presidential library opens in UNK , UNK underground cave',\n",
              " 'the incredible story of how an insurance company thinks a man burned his house down from UNK away',\n",
              " 'has prince william ever had a hot dog ? an investigation',\n",
              " 'sexy women UNK new life into coffin making business',\n",
              " 'doomsday clock pushed to one minute to UNK after arby ’ s threatens launch of UNK UNK beef ’ n bacon melt',\n",
              " 'charles UNK has ashes spread over UNK , UNK iraq',\n",
              " 'colorado legalizes UNK fireworks',\n",
              " 'un unveils design for floating city for 10,000 people',\n",
              " 'new bill would limit abortion to cases where procedure necessary to save promising political career',\n",
              " 'medical crisis : george h.w . bush has been rushed to the hospital for emergency lip UNK surgery',\n",
              " 'this ‘ smart condom ’ will give UNK into your sex life you probably didn ’ t want',\n",
              " 'gaffe as civil service magazine prints poster telling parents to shoot UNK children',\n",
              " \"it 's an UNK horror . a 14-year-old girl with special needs allegedly was raped at school after a teacher 's aide UNK her to act as UNK to catch an accused sexual predator , a fellow student .\",\n",
              " 'newly discovered cave paintings suggest early man was battling a lot of UNK demons',\n",
              " 'beijing declares scary halloween costumes illegal',\n",
              " 'jay-z vows not to lose touch with millionaire UNK on UNK throwback track about buying first yacht',\n",
              " 'UNK tight pants UNK sperm count',\n",
              " 'fat kid avoids UNK by swimming with shirt',\n",
              " 'severed head , UNK body found in same mississippi neighborhood',\n",
              " 'doctor who made music videos in UNK room facing several UNK suits',\n",
              " 'a publisher is turning the mueller report into a graphic novel',\n",
              " \"the us just UNK al-qaeda ’ s job application form . it 's UNK corporate .\",\n",
              " 'thanks for being so cool about everything -- vladimir putin',\n",
              " 'trump boys gather UNK of comic books , candy bars for night hiding from special prosecutors in UNK rose garden fort',\n",
              " 'mysterious UNK skin disease continues to eat away at baby ’ s face weeks after being kissed by ted cruz',\n",
              " 'pope francis tells survivors those who cover up abuse are ‘ s * * t ’',\n",
              " 'giant robot battle : who knew a duel between UNK UNK suits could be so boring ?',\n",
              " 'study UNK college education with brain tumor risk raises many questions',\n",
              " 'news : changing UNK : golden UNK is UNK itself as a cereal exclusively for people who are grieving',\n",
              " 'cracking story : french artist to UNK himself in rock for a week , then use body to hatch eggs',\n",
              " \"trump ca n't recall saying he has one of the world 's best memories\",\n",
              " 'every day , the tsa catches people smuggling UNK through x-ray machines . why aren ’ t they doing anything to stop it ?',\n",
              " 'trump UNK out at ap photographer who UNK empty chairs',\n",
              " 'UNK confirms up to 100 % UNK in beef products',\n",
              " 'obama : ‘ we tortured some folks ’',\n",
              " 'touching : the nra is releasing a UNK line of ar-15 rifles to raise money for the victims in parkland',\n",
              " 'UNK in korea cross UNK solo',\n",
              " 'kid rock apparently divorced UNK anderson because of UNK',\n",
              " '‘ i ’ m a UNK conservative , ’ says horrifying man 25 years from now',\n",
              " 'last thing government worker needed was agency labeling him ‘ UNK ’',\n",
              " 'theresa may : trump told me to sue the eu',\n",
              " 'drake slams rolling stone over losing cover to philip UNK UNK : ‘ i ’ m disgusted ’',\n",
              " 'congressman who UNK secret service was rejected by secret service',\n",
              " '‘ sometimes things have to get worse before they get better , ’ says man who accidentally turned shower UNK wrong way',\n",
              " 'no UNK ! government records UNK with UNK tape , paper',\n",
              " 'american baby names are somehow getting even worse',\n",
              " 'the onion is UNK at $ 500 UNK , more than many of the UNK it UNK ( x-post r / til )',\n",
              " 'massive semen explosion after blaze hits bull artificial UNK facility , firefighters forced to dodge “ UNK ”',\n",
              " 'gun control fail : this duck found a gun in a bush and is now pushing it around the park with its UNK',\n",
              " 'report : u.s. death rates from drugs , suicide , and alcohol have UNK increased , but not in a cool rock and roll way',\n",
              " 'man climbs on playground equipment to tell children where babies come from',\n",
              " 'a UNK , UNK UNK led to aliens : UNK marines ‘ weird ai',\n",
              " 'soccer UNK shot and killed after showing player red card',\n",
              " 'rats laugh when UNK UNK , top scientists reveal',\n",
              " 'lawyer for martin shkreli UNK fees five thousand per cent',\n",
              " 'hundreds of children terrified when movie theatre plays la UNK instead of detective pikachu',\n",
              " 'indiana governor insists new law has nothing to do with thing it UNK intended to do',\n",
              " 'UNK',\n",
              " 'all flights grounded after faa officials suddenly realize that man was not meant to fly',\n",
              " 'first family gets pet UNK',\n",
              " 'director seeking UNK unknown actress for next affair',\n",
              " 'UNK jewish newspaper UNK female world leaders out of charlie UNK march',\n",
              " 'life : this one ’ s on her : this woman gave more than $ 120,000 to an online dating scammer even though the guy had only UNK her ‘ hello ’ and never asked her for money',\n",
              " 'everyone is totally just UNK it , all the time',\n",
              " 'tinder users can now choose from 37 gender options',\n",
              " \"god cites UNK in mysterious ways ' as UNK in killing of 3,000 UNK new UNK\",\n",
              " \"peta crying UNK over signs that animals ca n't read .\",\n",
              " 'man tricked ex with abortion pill UNK',\n",
              " 'study links binge eating to stress , UNK , depression , joy , UNK , anger , UNK',\n",
              " 'woman feels UNK since growing out her beard , now looking for love',\n",
              " 'new UNK UNK UNK in face of UNK .',\n",
              " 'chicago police department to monitor all UNK with public using new bullet UNK',\n",
              " 'u.s. military defends controversial decision to test UNK volcano on UNK civilians',\n",
              " 'good samaritan : man shouts sex talk to boy stuck at bottom of well',\n",
              " 'a fifth of adults have forgotten how to do UNK or UNK',\n",
              " 'nba will consider UNK games due to UNK attention UNK',\n",
              " \"detectives UNK UNK anthony 's ' i killed my daughter ' UNK on reddit\",\n",
              " 'cleveland hero charles UNK rewarded with burgers for life',\n",
              " 'UNK family welcomes third child born on same day for third straight year',\n",
              " 'pimp my ride : hamas proudly shows off a tank , turns out to just be a car',\n",
              " 'mark UNK claims he would have hit 70 home runs without help of bat',\n",
              " 'death officially a motherfucker',\n",
              " 'grand jury indicted the man who filmed eric UNK ’ s killing',\n",
              " 'gop rep. steve king questions UNK UNK to civilization',\n",
              " \"new anti-smoking ads warn teens 'it 's gay to smoke '\",\n",
              " 'icy snowball can already tell it going to make 9-year-old cry',\n",
              " 'fbi warns ‘ UNK UNK ’ UNK could be target for shootings by UNK UNK',\n",
              " \"chuck e. cheese 's announces new lower prices , but the restaurants will be UNK\",\n",
              " 'tearful justify holds press conference blaming failed drug test on contaminated salt UNK']"
            ]
          },
          "execution_count": 172,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_train_loop_incorrect(model, test_iterator)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mUoLTGAScre7",
      "metadata": {
        "id": "mUoLTGAScre7"
      },
      "source": [
        "Now that we have our incorrect sequences:   \n",
        "**Question:** *Can you describe what cases the model is getting wrong in the witheld test-set?*\n",
        "\n",
        "**Answer:** The model seems to be replacing important entities/terms with 'UNK', due to low word frequency, which causes it to be unable to maintain context in sentences and hence incorrect outputs. This could point at a possibility where the cutoff was too high and key words were excluded from the vocab."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ie9VqRbg78Ty",
      "metadata": {
        "id": "Ie9VqRbg78Ty"
      },
      "source": [
        "## 6. LSTM Model [Extra credit, 4 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gXWSfPfBA4XU",
      "metadata": {
        "id": "gXWSfPfBA4XU"
      },
      "source": [
        "### 6.1 Define the RecurrentModel class\n",
        "Something that has been overlooked in this project (and a significant limitation of the bag-of-words approach) is the sequential structure of language: a word typically only has a clear meaning because of its relationship to the words before and after it in the sequence, and the feed-forward network of Part 2 cannot model this type of data. A solution to this, is the use of [recurrent neural networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/). These types of networks not only produce some output given some step from a sequence, but also update their internal state, hopefully \"remembering\" some information about the previous steps in the input sequence. Of course, they do have their own faults, but we'll cover this more thoroughly later in the semester.\n",
        "\n",
        "Your task for the extra credit portion of this assignment, is to implement such a model below using a LSTM. Instead of averaging the embeddings as with the FFN in Part 2, you'll instead feed all of these embeddings to a LSTM layer, get its final output, and use this to make your prediction for the class of the headline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "id": "YN8zvhLJ-MVJ",
      "metadata": {
        "id": "YN8zvhLJ-MVJ"
      },
      "outputs": [],
      "source": [
        "class RecurrentModel(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_dim, \\\n",
        "                num_layers=1, bidirectional=True):\n",
        "    \"\"\"\n",
        "    Instantiate layers for your model\n",
        "\n",
        "    Your model architecture will be an optionally bidirectional LSTM, followed\n",
        "    by a linear + sigmoid layer.\n",
        "\n",
        "    You will need 4 nn.Modules:\n",
        "      1. An embeddings layer (see nn.Embedding)\n",
        "      2. A bidirectional LSTM (see nn.LSTM)\n",
        "      3. A Linear layer (see nn.Linear)\n",
        "      4. A sigmoid output (see nn.Sigmoid)\n",
        "\n",
        "    HINT: In the forward step, the BATCH_SIZE is the first dimension.\n",
        "    HINT: Think about what happens to the linear layer's hidden_dim size\n",
        "          if bidirectional is True or False.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    ### BEGIN YOUR CODE (~4 lines) ###\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True, bidirectional=bidirectional)\n",
        "    self.linear = nn.Linear(2 * hidden_dim if bidirectional else hidden_dim, 1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    ### END YOUR CODE ###\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Complete the forward pass of the model.\n",
        "\n",
        "    Use the last timestep of the output of the LSTM as input to the linear\n",
        "    layer. This will only require some indexing into the correct return\n",
        "    from the LSTM layer.\n",
        "\n",
        "    Args:\n",
        "      x: 2D LongTensor of shape (BATCH_SIZE, max len of all tokenized_word_tensor))\n",
        "         This is the same output that comes out of the collate_fn function you completed-\n",
        "    \"\"\"\n",
        "    ### BEGIN YOUR CODE (~4-5 lines) ###\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    _, (hidden, _) = self.lstm(x)\n",
        "\n",
        "    if self.lstm.bidirectional:\n",
        "        last_state = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=-1)\n",
        "    else:\n",
        "        last_state = hidden[-1, :, :]\n",
        "    x = self.linear(last_state)\n",
        "    x = self.sigmoid(x)\n",
        "    return x\n",
        "    ### END YOUR CODE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HprkOm-fAVyj",
      "metadata": {
        "id": "HprkOm-fAVyj"
      },
      "source": [
        "Now that the `RecurrentModel` is defined, we will reinitialize our dataset iterators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "id": "6-uftfXEAqOi",
      "metadata": {
        "id": "6-uftfXEAqOi"
      },
      "outputs": [],
      "source": [
        "train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, collate_fn=collate_fn)\n",
        "val_iterator   = DataLoader(val_dataset, batch_size=BATCH_SIZE, sampler=val_sampler, collate_fn=collate_fn)\n",
        "test_iterator  = DataLoader(test_dataset, batch_size=BATCH_SIZE, sampler=test_sampler, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2qROtRw3AtZy",
      "metadata": {
        "id": "2qROtRw3AtZy"
      },
      "source": [
        "### 6.2 Initialize the LSTM classification model\n",
        "\n",
        "Next we need to initialize our new LSTM model, as well as define it's optimizer and loss function as we did for the FFNN. Feel free to use the same optimizer you did above, or see how this model reacts to different optimizers/learning rates than the FFNN.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "id": "LNWcLJpsBRzg",
      "metadata": {
        "id": "LNWcLJpsBRzg"
      },
      "outputs": [],
      "source": [
        "lstm_model = RecurrentModel(vocab_size    = len(train_vocab.keys()),\n",
        "                            embedding_dim = 300,\n",
        "                            hidden_dim    = 300,\n",
        "                            num_layers    = 1,\n",
        "                            bidirectional = True).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "id": "OdTxe0bFBqnP",
      "metadata": {
        "id": "OdTxe0bFBqnP"
      },
      "outputs": [],
      "source": [
        "lstm_criterion, lstm_optimizer = None, None\n",
        "\n",
        "### BEGIN YOUR CODE ###\n",
        "lstm_criterion = nn.BCELoss()\n",
        "lstm_optimizer = Adam(model.parameters())\n",
        "### END YOUR CODE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NFvV7H7OBzWl",
      "metadata": {
        "id": "NFvV7H7OBzWl"
      },
      "source": [
        "### 6.3 Training and Evaluation\n",
        "\n",
        "Because the only difference between this model and the FFN is the internal structure, we can use the same methods as above to evaluate and train it. You should be able to achieve a validation F-1 score of at least `0.80` if everything went correctly.\n",
        "\n",
        "**Feel free to adjust the number of epochs to prevent overfitting or underfitting and to play with your model hyperparameters/optimizer & loss function.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "id": "SdkEpedxDopv",
      "metadata": {
        "id": "SdkEpedxDopv"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 150/150 [00:02<00:00, 66.73it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Binary Macro F1: 0.2991609584156156\n",
            "Accuracy: 0.3858333333333333\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ===========================================================================\n",
        "# Pre-train to see what accuracy we can get with random parameters\n",
        "# ===========================================================================\n",
        "\n",
        "true, pred = val_loop(lstm_model, val_iterator)\n",
        "print(f'Binary Macro F1: {binary_macro_f1(true, pred)}')\n",
        "print(f'Accuracy: {accuracy(true, pred)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "id": "6p2dF9X4DyIR",
      "metadata": {
        "id": "6p2dF9X4DyIR"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1200/1200 [00:59<00:00, 20.22it/s]\n",
            "100%|██████████| 150/150 [00:02<00:00, 55.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH: 0\n",
            "TRAIN LOSS: 850.9629911780357\n",
            "VAL F-1: 0.2991609584156156\n",
            "VAL ACC: 0.3858333333333333\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1200/1200 [01:03<00:00, 18.89it/s]\n",
            "100%|██████████| 150/150 [00:02<00:00, 53.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH: 1\n",
            "TRAIN LOSS: 850.9629954695702\n",
            "VAL F-1: 0.2991609584156156\n",
            "VAL ACC: 0.3858333333333333\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1200/1200 [01:06<00:00, 18.14it/s]\n",
            "100%|██████████| 150/150 [00:03<00:00, 48.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH: 2\n",
            "TRAIN LOSS: 850.9629909396172\n",
            "VAL F-1: 0.2991609584156156\n",
            "VAL ACC: 0.3858333333333333\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1200/1200 [01:07<00:00, 17.69it/s]\n",
            "100%|██████████| 150/150 [00:02<00:00, 51.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH: 3\n",
            "TRAIN LOSS: 850.9629908800125\n",
            "VAL F-1: 0.2991609584156156\n",
            "VAL ACC: 0.3858333333333333\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1200/1200 [01:06<00:00, 18.17it/s]\n",
            "100%|██████████| 150/150 [00:03<00:00, 49.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH: 4\n",
            "TRAIN LOSS: 850.9629919528961\n",
            "VAL F-1: 0.2991609584156156\n",
            "VAL ACC: 0.3858333333333333\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1200/1200 [01:07<00:00, 17.75it/s]\n",
            "100%|██████████| 150/150 [00:02<00:00, 52.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH: 5\n",
            "TRAIN LOSS: 850.9629933834076\n",
            "VAL F-1: 0.2991609584156156\n",
            "VAL ACC: 0.3858333333333333\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1200/1200 [01:07<00:00, 17.69it/s]\n",
            "100%|██████████| 150/150 [00:02<00:00, 51.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH: 6\n",
            "TRAIN LOSS: 850.9629898071289\n",
            "VAL F-1: 0.2991609584156156\n",
            "VAL ACC: 0.3858333333333333\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1200/1200 [01:05<00:00, 18.45it/s]\n",
            "100%|██████████| 150/150 [00:02<00:00, 53.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH: 7\n",
            "TRAIN LOSS: 850.9629929661751\n",
            "VAL F-1: 0.2991609584156156\n",
            "VAL ACC: 0.3858333333333333\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1200/1200 [01:07<00:00, 17.88it/s]\n",
            "100%|██████████| 150/150 [00:02<00:00, 50.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH: 8\n",
            "TRAIN LOSS: 850.9629927277565\n",
            "VAL F-1: 0.2991609584156156\n",
            "VAL ACC: 0.3858333333333333\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1200/1200 [01:06<00:00, 18.16it/s]\n",
            "100%|██████████| 150/150 [00:02<00:00, 51.53it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH: 9\n",
            "TRAIN LOSS: 850.962993144989\n",
            "VAL F-1: 0.2991609584156156\n",
            "VAL ACC: 0.3858333333333333\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ===========================================================================\n",
        "# Train your LSTM model\n",
        "# ===========================================================================\n",
        "\n",
        "TOTAL_EPOCHS = 10\n",
        "for epoch in range(TOTAL_EPOCHS):\n",
        "    train_loss = train_loop(lstm_model, lstm_criterion, lstm_optimizer, train_iterator)\n",
        "    true, pred = val_loop(lstm_model, val_iterator)\n",
        "    print(f\"EPOCH: {epoch}\")\n",
        "    print(f\"TRAIN LOSS: {train_loss}\")\n",
        "    print(f\"VAL F-1: {binary_macro_f1(true, pred)}\")\n",
        "    print(f\"VAL ACC: {accuracy(true, pred)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "id": "OR8Dl5DLEQwd",
      "metadata": {
        "id": "OR8Dl5DLEQwd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 150/150 [00:02<00:00, 60.62it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TEST F-1: 0.288922015328918\n",
            "TEST ACC: 0.3745833333333333\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ===========================================================================\n",
        "# Evaluate your model on the held-out test set\n",
        "# ===========================================================================\n",
        "\n",
        "true, pred = val_loop(lstm_model, test_iterator)\n",
        "print(f\"TEST F-1: {binary_macro_f1(true, pred)}\")\n",
        "print(f\"TEST ACC: {accuracy(true, pred)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mY8S9ZK9zuVs",
      "metadata": {
        "id": "mY8S9ZK9zuVs"
      },
      "source": [
        "## 7. Submit Your Homework\n",
        "This is the end of Project 1. Congratulations!  \n",
        "\n",
        "Now, follow the steps below to submit your homework in [Gradescope](https://www.gradescope.com/courses/944807):\n",
        "\n",
        "1. Rename this ipynb file to `CS4650_p1_GTusername.ipynb`. Make sure all cells have been run. We recommend ensuring you have removed any extraneous cells & print statements, clearing all outputs, and using the Runtime --> Run all tool to make sure all output is update to date.\n",
        "2. Click on the menu 'File' --> 'Download' --> 'Download .py'.\n",
        "3. Click on the menu 'File' --> 'Download' --> 'Download .ipynb'.\n",
        "4. Download the notebook as a .pdf document. Make sure the output from Parts 4 & 6.3 are captured so we can see how the loss, F1, & accuracy changes while training.\n",
        "5. Upload all 3 files to Gradescope. Double check the files start with `CS4650_p1_*`, capitalization matters."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
